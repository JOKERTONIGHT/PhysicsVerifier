#!/usr/bin/env python3
"""
é€šç”¨ç‰©ç†ç«èµ›è¯„æµ‹è„šæœ¬ - æ”¯æŒå¤šç§æ¨¡å‹å’Œæ•°æ®æ ¼å¼

ä½¿ç”¨æ–¹æ³•:
    # è¯„æµ‹æ‰€æœ‰å¯ç”¨æ•°æ®é›†
    python eval_physics.py
    
    # è¯„æµ‹ç‰¹å®šæ•°æ®é›†
    python eval_physics.py --dataset panpho_2024
    
    # è¯„æµ‹JSONæ ¼å¼æ•°æ®é›†
    python eval_physics.py --dataset apho_2025_json
    
    # ä½¿ç”¨Judgeæ¨¡å‹è¿›è¡Œmarkingè¯„æµ‹ï¼ˆç»†ç²’åº¦ï¼‰
    python eval_physics.py --judge-model gpt-4o --api-key YOUR_API_KEY
    
    # æŒ‡å®šå¹¶è¡Œè¿›ç¨‹æ•°
    python eval_physics.py --nproc 8
    
    # ç¦ç”¨Judgeæ¨¡å‹ï¼ˆç»†ç²’åº¦å¾—åˆ†ä¸º0ï¼Œä»…ä½¿ç”¨ç­”æ¡ˆåŒ¹é…ï¼‰
    python eval_physics.py --no-judge
    
    # è¯„æµ‹å¤šæ¬¡è¿è¡Œç»“æœå¹¶è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    python eval_physics.py --multi-runs --dataset apho_2025
    
    # è¯„æµ‹æ‰€æœ‰æ•°æ®é›†çš„å¤šæ¬¡è¿è¡Œç»“æœ
    python eval_physics.py --multi-runs
    
    # æŒ‡å®šæ—¥å¿—ä¿å­˜ç›®å½•
    python eval_physics.py --log-dir my_logs

ä½œè€…: AI Assistant
æ—¶é—´: 2025å¹´
"""

import argparse
import os
import sys
import logging
from datetime import datetime
from pathlib import Path

# # æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°sys.path
# sys.path.append(str(Path(__file__).parent))

# åŠ è½½.envæ–‡ä»¶ä¸­çš„APIé…ç½®
from dotenv import load_dotenv
load_dotenv('.env')

from universal_physics_evaluator import UniversalPhysicsEvaluator
from variable_constant_verifier import VariableConstantVerifier
import json
from datetime import datetime as _dt

# å…¨å±€æ—¥å¿—è®°å½•å™¨
logger = None

def setup_logging(log_dir="logs", dataset_name=None, multi_runs=False):
    """è®¾ç½®æ—¥å¿—è®°å½•ç³»ç»Ÿ"""
    global logger
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    log_path = Path(log_dir)
    log_path.mkdir(parents=True, exist_ok=True)
    
    # ç”Ÿæˆæ—¥å¿—æ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    if dataset_name:
        dataset, model = dataset_name.split('/')
        if multi_runs:
            log_filename = f"eval_physics_multi_runs_{dataset}_{model}_{timestamp}.log"
        else:
            log_filename = f"eval_physics_{dataset_name}_{timestamp}.log"
    else:
        if multi_runs:
            log_filename = f"eval_physics_multi_runs_all_{timestamp}.log"
        else:
            log_filename = f"eval_physics_all_{timestamp}.log"
    
    log_file = log_path / log_filename
    
    # é…ç½®æ—¥å¿—è®°å½•å™¨
    logger = logging.getLogger('eval_physics')
    logger.setLevel(logging.INFO)
    
    # æ¸…é™¤å·²æœ‰çš„å¤„ç†å™¨
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    
    # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    
    # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    
    # åˆ›å»ºæ ¼å¼å™¨
    formatter = logging.Formatter('%(asctime)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
    file_handler.setFormatter(formatter)
    
    # æ§åˆ¶å°ä¸éœ€è¦æ—¶é—´æˆ³æ ¼å¼å™¨ï¼Œä¿æŒåŸæœ‰è¾“å‡ºæ ¼å¼
    console_formatter = logging.Formatter('%(message)s')
    console_handler.setFormatter(console_formatter)
    
    # æ·»åŠ å¤„ç†å™¨åˆ°æ—¥å¿—è®°å½•å™¨
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return log_file

def log_print(*args, **kwargs):
    """æ›¿ä»£safe_printçš„æ—¥å¿—æ‰“å°å‡½æ•°"""
    global logger
    message = ' '.join(str(arg) for arg in args)
    if logger:
        logger.info(message)
    else:
        print(message, **kwargs)

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="é€šç”¨ç‰©ç†ç«èµ›è¯„æµ‹è„šæœ¬ - æ”¯æŒå¤šç§æ¨¡å‹å’Œæ•°æ®æ ¼å¼")
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument(
        "--results-dir", 
        type=str, 
        default="results_reasoning",
        help="æ¨ç†ç»“æœç›®å½• (é»˜è®¤: results)"
    )
    
    parser.add_argument(
        "--dataset", 
        type=str, 
        help="æŒ‡å®šè¦è¯„æµ‹çš„æ•°æ®é›† (å¦‚æœä¸æŒ‡å®šï¼Œåˆ™è¯„æµ‹æ‰€æœ‰å¯ç”¨æ•°æ®é›†)"
    )
    
    parser.add_argument(
        "--nproc", 
        type=int, 
        default=8,
        help="å¹¶è¡Œè¿›ç¨‹æ•° (é»˜è®¤: 8)"
    )
    
    # Judgeæ¨¡å‹å‚æ•°
    parser.add_argument(
        "--judge-model", 
        type=str, 
        default="gemini-2.5-flash",
        help="Judgeæ¨¡å‹åç§° (ä¾‹å¦‚: gpt-4o, gpt-4-turbo, claude-3-sonnet-20240229)"
    )
    
    parser.add_argument(
        "--api-key", 
        type=str, 
        help="APIå¯†é’¥ (å¯é€‰ï¼Œä¹Ÿå¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡è®¾ç½®)"
    )
    
    parser.add_argument(
        "--no-judge", 
        action="store_true",
        help="ç¦ç”¨Judgeæ¨¡å‹ï¼ˆç»†ç²’åº¦å¾—åˆ†ä¸º0ï¼Œä»è¿›è¡Œç²—ç²’åº¦è¯„æµ‹ï¼‰"
    )
    
    parser.add_argument(
        "--multi-runs", 
        action="store_true",
        help="è¯„æµ‹å¤šæ¬¡è¿è¡Œç»“æœå¹¶è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"
    )
    
    # å…¶ä»–å‚æ•°
    parser.add_argument(
        "--output-dir", 
        type=str, 
        default="evaluation_results",
        help="è¯„æµ‹ç»“æœè¾“å‡ºç›®å½• (é»˜è®¤: evaluation_results)"
    )
    
    parser.add_argument(
        "--verbose", 
        action="store_true",
        help="è¯¦ç»†è¾“å‡ºæ¨¡å¼"
    )
    
    parser.add_argument(
        "--dry-run", 
        action="store_true",
        help="è¯•è¿è¡Œæ¨¡å¼ï¼Œä»…æ£€æŸ¥æ•°æ®é›†è€Œä¸è¿›è¡Œå®é™…è¯„æµ‹"
    )
    
    parser.add_argument(
        "--log-dir", 
        type=str, 
        default="logs",
        help="æ—¥å¿—æ–‡ä»¶ä¿å­˜ç›®å½• (é»˜è®¤: logs)"
    )

    # å˜é‡/å¸¸é‡æ··æ·†æ£€æŸ¥å™¨å‚æ•°
    parser.add_argument(
        "--varconst-check",
        action="store_true",
        help="å¯ç”¨å˜é‡/å¸¸é‡æ··æ·†æ£€æŸ¥å™¨ï¼Œå¯¹æ¨¡å‹ä½œç­”è¿›è¡Œé™æ€+å¯é€‰LLMè¾…åŠ©çš„ç¬¦å·ä¸€è‡´æ€§æ£€æŸ¥"
    )
    parser.add_argument(
        "--varconst-llm-model",
        type=str,
        default=None,
        help="ç”¨äºè¾…åŠ©ç­‰ä»·æ€§/é€‚ç”¨æ€§åˆ¤æ–­çš„LLMæ¨¡å‹æ ‡è¯†ï¼ˆå¯é€‰ï¼‰ã€‚ç•™ç©ºåˆ™ä»…ä½¿ç”¨è§„åˆ™æ£€æŸ¥"
    )
    parser.add_argument(
        "--varconst-max-llm-calls",
        type=int,
        default=0,
        help="å¯ç”¨äºæ£€æŸ¥å™¨çš„LLMè°ƒç”¨ä¸Šé™ï¼ˆé»˜è®¤0=ä¸è°ƒç”¨ï¼‰"
    )
    
    return parser.parse_args()

def setup_environment(args):
    """è®¾ç½®è¿è¡Œç¯å¢ƒ"""
    # è®¾ç½®APIå¯†é’¥
    if args.api_key:
        os.environ["OPENAI_API_KEY"] = args.api_key
        log_print(f"âœ… å·²è®¾ç½®APIå¯†é’¥")
    
    # æ£€æŸ¥ç»“æœç›®å½•
    results_dir = Path(args.results_dir)
    if not results_dir.exists():
        log_print(f"âŒ ç»“æœç›®å½•ä¸å­˜åœ¨: {results_dir}")
        sys.exit(1)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    log_print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")

def build_judge_kwargs(args):
    """æ„å»ºJudgeæ¨¡å‹å‚æ•°"""
    judge_kwargs = {}
    
    if args.no_judge:
        log_print("âš ï¸  ç¦ç”¨Judgeæ¨¡å‹ï¼Œç»†ç²’åº¦å¾—åˆ†å°†ä¸º0ï¼Œä»è¿›è¡Œç²—ç²’åº¦è¯„æµ‹")
        return judge_kwargs
    
    if args.judge_model:
        judge_kwargs['model'] = args.judge_model
        log_print(f"ğŸ¤– ä½¿ç”¨Judgeæ¨¡å‹: {args.judge_model}")
        
        # æ£€æŸ¥APIå¯†é’¥
        if not os.getenv("OPENAI_API_KEY") and not args.api_key:
            log_print("âš ï¸  è­¦å‘Š: æœªè®¾ç½®APIå¯†é’¥ï¼ŒJudgeæ¨¡å‹å¯èƒ½æ— æ³•å·¥ä½œ")
            log_print("   è¯·é€šè¿‡ --api-key å‚æ•°æˆ– OPENAI_API_KEY ç¯å¢ƒå˜é‡è®¾ç½®")
    else:
        log_print("â„¹ï¸  æœªæŒ‡å®šJudgeæ¨¡å‹ï¼Œç»†ç²’åº¦å¾—åˆ†å°†ä¸º0ï¼Œä»…ä½¿ç”¨ç²—ç²’åº¦è¯„æµ‹")
    
    if args.nproc:
        judge_kwargs['nproc'] = args.nproc
    
    return judge_kwargs

def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    # è®¾ç½®æ—¥å¿—ç³»ç»Ÿ
    log_file = setup_logging(
        log_dir=args.log_dir, 
        dataset_name=args.dataset, 
        multi_runs=args.multi_runs
    )
    
    # æ‰“å°å¯åŠ¨ä¿¡æ¯
    log_print("ğŸš€ é€šç”¨ç‰©ç†ç«èµ›è¯„æµ‹ç³»ç»Ÿå¯åŠ¨")
    log_print("=" * 60)
    log_print(f"ğŸ“‚ æ¨ç†ç»“æœç›®å½•: {args.results_dir}")
    log_print(f"ğŸ“Š å¹¶è¡Œè¿›ç¨‹æ•°: {args.nproc}")
    log_print(f"ğŸ’¾ è¾“å‡ºç›®å½•: {args.output_dir}")
    log_print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_file}")
    if args.dataset:
        log_print(f"ğŸ¯ æŒ‡å®šæ•°æ®é›†: {args.dataset}")
    else:
        log_print(f"ğŸ¯ è¯„æµ‹æ¨¡å¼: æ‰€æœ‰å¯ç”¨æ•°æ®é›†")
    log_print("=" * 60)
    
    # è®¾ç½®ç¯å¢ƒ
    setup_environment(args)
    
    # æ„å»ºJudgeå‚æ•°
    judge_kwargs = build_judge_kwargs(args)
    
    # åˆå§‹åŒ–è¯„æµ‹å™¨
    evaluator = UniversalPhysicsEvaluator(
        args.results_dir,
        args.output_dir,
        nproc=args.nproc
    )
    log_print("âœ… è¯„æµ‹å™¨åˆå§‹åŒ–æˆåŠŸ")

    def _run_varconst_for_dataset(ds_key: str, run_payload: dict = None):
        """å¯¹æŒ‡å®šæ•°æ®é›†ï¼ˆå¯é€‰æŒ‡å®šå•æ¬¡è¿è¡Œæ•°æ®ï¼‰æ‰§è¡Œå˜é‡/å¸¸é‡æ£€æŸ¥å¹¶ä¿å­˜æŠ¥å‘Šã€‚

        run_payload: å¯é€‰ï¼Œå½¢å¦‚ {"run_name": str, "samples": List[Dict]}ï¼Œè‹¥æä¾›åˆ™ç›´æ¥ä½¿ç”¨æ ·æœ¬åˆ—è¡¨ï¼›
                     å¦åˆ™é€šè¿‡ evaluator åŠ è½½è¯¥æ•°æ®é›†çš„æ¨ç†ç»“æœã€‚
        """
        if not args.varconst_check:
            return
        try:
            verifier = VariableConstantVerifier(
                llm_model=args.varconst_llm_model,
                max_llm_calls=args.varconst_max_llm_calls,
                logger=logger,
            )

            # åŠ è½½æ ·æœ¬
            samples = None
            run_name = None
            if run_payload and isinstance(run_payload, dict):
                samples = run_payload.get("samples")
                run_name = run_payload.get("run_name")
            if samples is None:
                # å°è¯•é€šè¿‡ evaluator åŠ è½½
                load_fn = getattr(evaluator, "load_inference_results", None)
                if callable(load_fn):
                    samples = load_fn(ds_key)
                else:
                    log_print(f"âš ï¸ æ— æ³•åŠ è½½æ¨ç†ç»“æœä»¥è¿è¡Œå˜é‡/å¸¸é‡æ£€æŸ¥ï¼ˆç¼ºå°‘ load_inference_results æ–¹æ³•ï¼‰: {ds_key}")
                    return

            # è§„èŒƒåŒ–æ ·æœ¬å­—æ®µï¼ŒæŒ‘é€‰ question/context/prediction/id
            norm_samples = []
            for r in samples or []:
                norm_samples.append({
                    "id": (r.get("id") or r.get("problem_id") or r.get("sample_id") or "").strip(),
                    "question": r.get("question") or r.get("prompt") or r.get("instruction"),
                    "context": r.get("context") or r.get("passage") or r.get("materials"),
                    "prediction": r.get("prediction") or r.get("output") or r.get("final_answer") or r.get("answer_text") or "",
                    "meta": {k: v for k, v in r.items() if k not in {"id","problem_id","sample_id","question","prompt","instruction","context","passage","materials","prediction","output","final_answer","answer_text"}}
                })

            report = verifier.analyze_batch(norm_samples, dataset_key=ds_key)

            # ä¿å­˜æŠ¥å‘Š
            out_dir = Path(args.output_dir) / "varconst_reports"
            out_dir.mkdir(parents=True, exist_ok=True)
            ts = _dt.now().strftime("%Y%m%d_%H%M%S")
            base = ds_key.replace("/", "_")
            if run_name:
                out_path = out_dir / f"{base}__{run_name}__varconst_{ts}.json"
            else:
                out_path = out_dir / f"{base}__varconst_{ts}.json"
            with out_path.open("w", encoding="utf-8") as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            log_print(f"ğŸ§ª å˜é‡/å¸¸é‡æ£€æŸ¥å®Œæˆï¼ŒæŠ¥å‘Šå·²ä¿å­˜: {out_path}")
        except Exception as e:
            log_print(f"âŒ å˜é‡/å¸¸é‡æ£€æŸ¥å¤±è´¥ï¼ˆ{ds_key}ï¼‰: {e}")
    
    # è¯•è¿è¡Œæ¨¡å¼ï¼šä»…æ£€æŸ¥æ•°æ®é›†
    if args.dry_run:
        log_print("\nğŸ” è¯•è¿è¡Œæ¨¡å¼ï¼šæ£€æŸ¥å¯ç”¨æ•°æ®é›†...")
        available_datasets = evaluator.detect_available_datasets()
        log_print(f"\nğŸ“Š å‘ç° {len(available_datasets)} ä¸ªå¯ç”¨æ•°æ®é›†:")
        for dataset_key in available_datasets:
            config = evaluator.DATASET_CONFIGS[dataset_key]
            log_print(f"   âœ“ {config['display_name']} ({dataset_key})")
        log_print("\nâœ… è¯•è¿è¡Œå®Œæˆ")
        return
    
    # å¼€å§‹è¯„æµ‹
    if args.multi_runs:
        # å¤šæ¬¡è¿è¡Œè¯„æµ‹æ¨¡å¼
        if args.dataset:
            # è¯„æµ‹å•ä¸ªæ•°æ®é›†çš„å¤šæ¬¡è¿è¡Œ
            log_print(f"\nğŸ”„ å¼€å§‹å¤šæ¬¡è¿è¡Œè¯„æµ‹: {args.dataset}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¤šæ¬¡è¿è¡Œ
            if not evaluator.has_multiple_runs(args.dataset):
                log_print(f"âš ï¸  æ•°æ®é›† {args.dataset} æ²¡æœ‰å¤šæ¬¡è¿è¡Œç»“æœ")
                return
            
            multi_run_results = evaluator.evaluate_multiple_runs(args.dataset, judge_kwargs)
            
            if multi_run_results:
                overall = multi_run_results['overall_statistics']
                log_print(f"\nğŸ‰ å¤šæ¬¡è¿è¡Œè¯„æµ‹å®Œæˆï¼")
                log_print(f"ğŸ”„ è¿è¡Œæ¬¡æ•°: {overall['num_runs']}")
                log_print(f"ğŸ“ˆ å¹³å‡å¾—åˆ†ç‡: {overall['mean_score_rate']:.2f}% Â± {overall['std_score_rate']:.2f}%")
                # åŒæ­¥è¿è¡Œå˜é‡/å¸¸é‡æ£€æŸ¥ï¼šå¯¹æ¯ä¸ª run å•ç‹¬ç”ŸæˆæŠ¥å‘Š
                if args.varconst_check:
                    load_multi = getattr(evaluator, "load_multiple_runs_results", None)
                    if callable(load_multi):
                        runs_map = load_multi(args.dataset)
                        for run_name, samples in (runs_map or {}).items():
                            _run_varconst_for_dataset(args.dataset, {"run_name": run_name, "samples": samples})
                    else:
                        log_print("âš ï¸ æœªæä¾› load_multiple_runs_resultsï¼Œå˜é‡/å¸¸é‡æ£€æŸ¥ä»…å¯¹å•æ¬¡åŠ è½½æ”¯æŒ")
            else:
                log_print(f"âŒ æ•°æ®é›† {args.dataset} å¤šæ¬¡è¿è¡Œè¯„æµ‹å¤±è´¥")
        else:
            # è¯„æµ‹æ‰€æœ‰æ•°æ®é›†çš„å¤šæ¬¡è¿è¡Œ
            log_print(f"\nğŸŒŸ å¼€å§‹è¯„æµ‹æ‰€æœ‰æ•°æ®é›†çš„å¤šæ¬¡è¿è¡Œ...")
            available_datasets = evaluator.detect_available_datasets()
            
            multi_run_datasets = []
            for dataset_key in available_datasets:
                if evaluator.has_multiple_runs(dataset_key):
                    multi_run_datasets.append(dataset_key)
            
            if not multi_run_datasets:
                log_print(f"âŒ æœªå‘ç°ä»»ä½•å…·æœ‰å¤šæ¬¡è¿è¡Œç»“æœçš„æ•°æ®é›†")
                return
            
            log_print(f"ğŸ“Š å‘ç° {len(multi_run_datasets)} ä¸ªå…·æœ‰å¤šæ¬¡è¿è¡Œçš„æ•°æ®é›†")
            
            all_multi_run_results = {}
            for dataset_key in multi_run_datasets:
                log_print(f"\n{'='*60}")
                log_print(f"ğŸ”„ è¯„æµ‹å¤šæ¬¡è¿è¡Œ: {dataset_key}")
                log_print(f"{'='*60}")
                
                
                multi_run_results = evaluator.evaluate_multiple_runs(dataset_key, judge_kwargs)
                all_multi_run_results[dataset_key] = multi_run_results
                
                if multi_run_results:
                    overall = multi_run_results['overall_statistics']
                    log_print(f"âœ… å®Œæˆ: å¹³å‡å¾—åˆ†ç‡ {overall['mean_score_rate']:.2f}% Â± {overall['std_score_rate']:.2f}%")
                    # å˜é‡/å¸¸é‡æ£€æŸ¥ï¼šé€ run æŠ¥å‘Š
                    if args.varconst_check:
                        load_multi = getattr(evaluator, "load_multiple_runs_results", None)
                        if callable(load_multi):
                            runs_map = load_multi(dataset_key)
                            for run_name, samples in (runs_map or {}).items():
                                _run_varconst_for_dataset(dataset_key, {"run_name": run_name, "samples": samples})
            
            # ä¿å­˜æ‰€æœ‰å¤šæ¬¡è¿è¡Œç»“æœçš„æ±‡æ€»
            if all_multi_run_results:
                evaluator._save_all_multi_run_summary(all_multi_run_results)
                
    else:
        # å¸¸è§„è¯„æµ‹æ¨¡å¼
        if args.dataset:
            # è¯„æµ‹å•ä¸ªæ•°æ®é›†
            log_print(f"\nğŸ¯ å¼€å§‹è¯„æµ‹å•ä¸ªæ•°æ®é›†: {args.dataset}")
            results = evaluator.evaluate_dataset(args.dataset, judge_kwargs)
            
            if results:
                config = evaluator.DATASET_CONFIGS[args.dataset]
                log_print(f"\nâœ… {config['display_name']} è¯„æµ‹å®Œæˆï¼")
                log_print(f"ğŸ† æ€»ä½“å¾—åˆ†: {results['total_score']:.2f} / {results['max_possible_score']:.2f} ({results['score_rate']:.2f}%)")
                _run_varconst_for_dataset(args.dataset)
            else:
                log_print(f"âŒ æ•°æ®é›† {args.dataset} è¯„æµ‹å¤±è´¥")
        else:
            # è¯„æµ‹æ‰€æœ‰æ•°æ®é›†
            log_print(f"\nğŸŒŸ å¼€å§‹è¯„æµ‹æ‰€æœ‰å¯ç”¨æ•°æ®é›†...")
            all_results = evaluator.evaluate_all_datasets(judge_kwargs)
            
            if all_results:
                log_print(f"\nğŸ‰ æ‰€æœ‰æ•°æ®é›†è¯„æµ‹å®Œæˆï¼")
                successful_count = sum(1 for r in all_results.values() if r is not None)
                log_print(f"ğŸ“Š æˆåŠŸè¯„æµ‹ {successful_count}/{len(all_results)} ä¸ªæ•°æ®é›†")
                # å¯¹æˆåŠŸçš„é›†åˆè¿è¡Œå˜é‡/å¸¸é‡æ£€æŸ¥
                if args.varconst_check:
                    for ds_key, r in (all_results or {}).items():
                        if r is not None:
                            _run_varconst_for_dataset(ds_key)
            else:
                log_print(f"âŒ æœªèƒ½æˆåŠŸè¯„æµ‹ä»»ä½•æ•°æ®é›†")
    
    log_print(f"\nğŸ¯ è¯„æµ‹å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {args.output_dir}")
    log_print(f"ğŸ“ å®Œæ•´æ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}")

if __name__ == "__main__":
    main()