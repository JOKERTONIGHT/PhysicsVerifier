import os
import re
import pandas as pd
import json
import glob
import warnings
import time
import threading
import datetime
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional, Union, Any

# å¯¼å…¥VLMEvalKitçš„ç›¸å…³å·¥å…·
# import sys
# sys.path.append('VLMEvalKit')

# åŠ è½½.envæ–‡ä»¶ä¸­çš„APIé…ç½®
from dotenv import load_dotenv
load_dotenv('.env')

from vlmeval.smp import load, dump, gpt_key_set
from vlmeval.dataset.physics_r1 import grade, extract_boxed_answer, get_answer_str, answer_tag_reward_fn_for_r1
from vlmeval.dataset.utils import build_judge
from vlmeval.utils import track_progress_rich

# çº¿ç¨‹é”ç”¨äºåŒæ­¥è¾“å‡º
output_lock = threading.Lock()

def safe_print(*args, **kwargs):
    """çº¿ç¨‹å®‰å…¨çš„æ‰“å°å‡½æ•°"""
    with output_lock:
        print(*args, **kwargs)

class LogBuffer:
    """æ—¥å¿—ç¼“å­˜ç±»ï¼Œç”¨äºæ”¶é›†å•ä¸ªä»»åŠ¡çš„æ‰€æœ‰æ—¥å¿—"""
    def __init__(self, task_id):
        self.task_id = task_id
        self.logs = []
        self.start_time = datetime.datetime.now()
    
    def log(self, message):
        """æ·»åŠ æ—¥å¿—æ¶ˆæ¯"""
        timestamp = datetime.datetime.now().strftime("%H:%M:%S.%f")[:-3]
        self.logs.append(f"[{timestamp}] [{self.task_id}] {message}")
    
    def flush(self):
        """ä¸€æ¬¡æ€§è¾“å‡ºæ‰€æœ‰ç¼“å­˜çš„æ—¥å¿—"""
        with output_lock:
            for log in self.logs:
                print(log)
            print()  # æ·»åŠ ç©ºè¡Œåˆ†éš”ä¸åŒä»»åŠ¡çš„è¾“å‡º


class UniversalPhysicsEvaluator:
    """
    é€šç”¨ç‰©ç†ç«èµ›è¯„æµ‹ç³»ç»Ÿ
    
    æ”¯æŒå¤šç§ç‰©ç†ç«èµ›æ•°æ®é›†çš„è‡ªåŠ¨è¯„æµ‹ï¼š
    - PanPhO2024/2025: æ³›äºšç‰©ç†å¥¥æ—åŒ¹å…‹
    - IPhO2024/2025: å›½é™…ç‰©ç†å¥¥æ—åŒ¹å…‹
    - EuPhO2024/2025: æ¬§æ´²ç‰©ç†å¥¥æ—åŒ¹å…‹
    - APhO2025: äºšæ´²ç‰©ç†å¥¥æ—åŒ¹å…‹
    - FMA2024/2025: æ³•å›½æ•°å­¦ç«èµ›
    - PanPhO_Mechanics2024/2025: åŠ›å­¦ä¸“é¡¹
    
    æ”¯æŒç»†ç²’åº¦å’Œç²—ç²’åº¦è¯„æµ‹
    """
    
    # æ•°æ®é›†é…ç½®æ˜ å°„
    CONFIGS = {
        'datasets': ['IPhO_2025', 'IPhO_2024', 'APhO_2025', 'EuPhO_2025', 'EuPhO_2024', 'FMA_2024', 'FMA_2025', 'NBPhO_2024', 'NBPhO_2025', 'PanPhO_2024', 'PanPhO_2025', 'PanMechanics_2024', 'PanMechanics_2025'],
        'models': ['qwen3-vl-235b-a22b-thinking', 'Physics-235B-1001', 'Physics-235B-0929', 'Physics-30B-0929', 'Gemini-2.5-Flash-Thinking', 'Gemini-2.5-Pro-Thinking', 'gpt-5-2025-08-07', 'deepseek-r1', 'deepseek-v3', 'qwen3-8b', 'qwen3-30b', 'qwen3-32b', 'qwen3-235b', 'Physics-p2', 'Physics-0915', 'Qwen3-30B', 'Qwen3-235B', 'Qwen-235B-2507', 'Physics-30B-0915', 'Physics-0921', 'Physics-0923', 'Physics-119'] 
    }

    def __init__(self, infer_dir="results_reasoning_no_thinking", eval_dir="evaluation_results_no_thinking", nproc: int = 4):
        """
        åˆå§‹åŒ–è¯„æµ‹å™¨
        
        Args:
            infer_dir: æ¨ç†ç»“æœç›®å½•
            eval_dir: è¯„æµ‹ç»“æœç›®å½•
            nproc: å¹¶è¡Œè¿›ç¨‹æ•°
        """
        self.infer_dir = Path(infer_dir)
        self.eval_dir = Path(eval_dir)  
        self.nproc = nproc
        
        if not self.infer_dir.exists():
            raise FileNotFoundError(f"Inference directory not found: {self.infer_dir}")
        if not self.eval_dir.exists():
            raise FileNotFoundError(f"Evaluation directory not found: {self.eval_dir}")
    
    def detect_available_datasets(self) -> List[str]:
        """æ£€æµ‹å¯ç”¨çš„æ•°æ®é›†"""
        available_datasets = []

        for dataset in self.CONFIGS['datasets']:
            for model in self.CONFIGS['models']:
                infer_dir = self.infer_dir / dataset / model
                eval_dir = self.eval_dir / dataset / model
                if infer_dir.exists() and not eval_dir.exists():
                    available_datasets.append(f"{dataset}/{model}")
        return available_datasets

    def detect_multiple_runs(self, dataset_key: str) -> List[str]:
        """æ£€æµ‹æ•°æ®é›†çš„å¤šæ¬¡è¿è¡Œç»“æœ""" 
        dataset, model = dataset_key.split('/')
        result_dir = self.infer_dir / dataset / model
        run_files = list(result_dir.glob("*.json")) # run_files type: list[PosixPath]
        run_files = [str(run_file) for run_file in run_files]
        return run_files

    def has_multiple_runs(self, dataset_key: str) -> bool:
        """æ£€æŸ¥æ•°æ®é›†æ˜¯å¦æœ‰å¤šæ¬¡è¿è¡Œç»“æœ"""
        return len(self.detect_multiple_runs(dataset_key)) > 1

    def load_inference_results(self, dataset_key: str, infer_file: Optional[str] = None) -> List[Dict]:
        """åŠ è½½æ¨ç†ç»“æœ"""
        dataset, model = dataset_key.split('/')
        result_dir = self.infer_dir / dataset / model
        safe_print(f"ğŸ“ åŠ è½½æ¨ç†ç»“æœ: {result_dir}")
        
        if infer_file:
            # åŠ è½½æŒ‡å®šçš„è¿è¡Œç»“æœ
            inference_file = infer_file
            if not os.path.exists(inference_file):
                raise FileNotFoundError(f"No inference_results.json found in {result_dir}/{infer_file}.json")
            safe_print(f"   ä½¿ç”¨æŒ‡å®šæ–‡ä»¶: {inference_file}")
        else:
            # æŸ¥æ‰¾run_*ç›®å½•ä¸­çš„inference_results.jsonæ–‡ä»¶
            inference_files = list(result_dir.glob("*.json"))
            if not inference_files:
                raise FileNotFoundError(f"No *.json files found in {result_dir}")
            
            # åŠ è½½æœ€æ–°çš„æ¨ç†ç»“æœ
            inference_file = max(inference_files, key=lambda x: x.parent.name)
            safe_print(f"   ä½¿ç”¨æœ€æ–°æ–‡ä»¶: {inference_file}")
        
        with open(inference_file, 'r', encoding='utf-8') as f:
            results = json.load(f)
        
        safe_print(f"   æ¨ç†ç»“æœæ•°é‡: {len(results)}")
        return results

    def load_multiple_runs_results(self, dataset_key: str) -> Dict[str, List[Dict]]:
        """åŠ è½½å¤šæ¬¡è¿è¡Œçš„æ‰€æœ‰æ¨ç†ç»“æœ"""
        run_files = self.detect_multiple_runs(dataset_key)
        if not run_files:
            raise ValueError(f"No multiple runs found for dataset: {dataset_key}")
        
        all_runs_results = {}
        for run_file in run_files:
            try:
                results = self.load_inference_results(dataset_key, run_file)
                all_runs_results[str(run_file)] = results
                safe_print(f"âœ… åŠ è½½ {run_file}: {len(results)} æ¡ç»“æœ")
            except Exception as e:
                safe_print(f"âš ï¸  è·³è¿‡ {run_file}: {e}")
        
        return all_runs_results

    def prepare_evaluation_data(self, dataset_key: str) -> pd.DataFrame:
        """å‡†å¤‡è¯„æµ‹æ•°æ®ï¼Œç›´æ¥ä»JSONæ–‡ä»¶åŠ è½½"""
        # ç›´æ¥åŠ è½½æ¨ç†ç»“æœJSONï¼Œé‡Œé¢å·²ç»åŒ…å«äº†æ‰€æœ‰éœ€è¦çš„å­—æ®µ
        inference_results = self.load_inference_results(dataset_key)
        
        # è½¬æ¢ä¸ºDataFrame
        eval_data = pd.DataFrame(inference_results)
        
        safe_print(f"âœ… ç›´æ¥åŠ è½½è¯„æµ‹æ•°æ®ï¼Œå…± {len(eval_data)} æ¡è®°å½•")
        safe_print(f"ğŸ“Š æ•°æ®åˆ—å: {list(eval_data.columns)}")
        
        # æ£€æŸ¥å¿…è¦å­—æ®µ
        required_fields = ['prediction', 'answer']
        for field in required_fields:
            if field not in eval_data.columns:
                raise ValueError(f"Missing required field: {field}")
        
        return eval_data

    def _safe_parse_json_field(self, field_value):
        """å®‰å…¨è§£æJSONå­—æ®µ"""
        # å¦‚æœå·²ç»æ˜¯åˆ—è¡¨ï¼Œç›´æ¥è¿”å›
        if isinstance(field_value, list):
            return field_value
        
        # æ£€æŸ¥Noneå’ŒNaN
        if field_value is None:
            return []
        
        try:
            if pd.isna(field_value):
                return []
        except (TypeError, ValueError):
            # å¤„ç†æ— æ³•ç”¨pd.isnaæ£€æŸ¥çš„æƒ…å†µ
            pass
        
        if field_value == '':
            return []
        
        field_str = str(field_value).strip()
        if field_str.startswith('[') and field_str.endswith(']'):
            try:
                return json.loads(field_str)
            except json.JSONDecodeError:
                return [field_str]
        else:
            return [field_str] if field_str != 'nan' else []
    
    def _safe_parse_points_field(self, points_value):
        """å®‰å…¨è§£æpointså­—æ®µ"""
        # å¦‚æœå·²ç»æ˜¯åˆ—è¡¨ï¼Œç›´æ¥è½¬æ¢
        if isinstance(points_value, list):
            return [float(p) for p in points_value if p is not None]
        
        # æ£€æŸ¥None
        if points_value is None:
            return [0.0]
            
        # æ£€æŸ¥NaN
        try:
            if pd.isna(points_value):
                return [0.0]
        except (TypeError, ValueError):
            # å¤„ç†æ— æ³•ç”¨pd.isnaæ£€æŸ¥çš„æƒ…å†µ
            pass
        
        if isinstance(points_value, (int, float)):
            return [float(points_value)]
        
        points_str = str(points_value).strip()
        if points_str.startswith('[') and points_str.endswith(']'):
            try:
                parsed = json.loads(points_str)
                return [float(p) for p in parsed if p is not None]
            except (json.JSONDecodeError, ValueError):
                pass
        
        try:
            return [float(points_str)]
        except ValueError:
            return [0.0]

    def _has_valid_marking(self, marking):
        """æ£€æŸ¥markingæ˜¯å¦åŒ…å«æœ‰æ•ˆçš„è¯„åˆ†æ ‡å‡†"""
        if not marking:
            return False
        
        if not isinstance(marking, list):
            return False
        
        if len(marking) == 0:
            return False
        
        for item in marking:
            if item is None:
                continue
            
            if isinstance(item, list):
                if len(item) > 0:
                    return True
            elif isinstance(item, str):
                stripped = item.strip()
                if stripped and stripped.lower() not in ['', 'nan', 'none', 'null']:
                    return True
            else:
                return True
        
        return False

    def evaluate_dataset(self, dataset_key: str, judge_kwargs: Optional[Dict] = None) -> Dict:
        """è¯„æµ‹å•ä¸ªæ•°æ®é›†"""
        if judge_kwargs is None:
            judge_kwargs = {}
        
        safe_print(f"ğŸš€ å¼€å§‹è¯„æµ‹æ•°æ®é›†: {dataset_key}")
        
        # å‡†å¤‡è¯„æµ‹æ•°æ®
        eval_data = self.prepare_evaluation_data(dataset_key)
        
        # åˆå§‹åŒ–judgeæ¨¡å‹
        judge_model = self._init_judge_model(judge_kwargs)
        
        safe_print(f"ğŸ“Š å¼€å§‹å¹¶è¡Œè¯„æµ‹ï¼Œå…±{len(eval_data)}é¢˜...")
        
        # æ„å»ºä»»åŠ¡åˆ—è¡¨
        tasks = []
        indices = []
        for i in range(len(eval_data)):
            row = eval_data.iloc[i]
            task_kwargs = judge_kwargs.copy()
            task = (judge_model, row, i, task_kwargs)
            tasks.append(task)
            indices.append(i)
        
        safe_print(f"ğŸ”„ å¯åŠ¨å¹¶è¡Œè¯„æµ‹ï¼Œä»»åŠ¡æ•°: {len(tasks)}")
        
        # è®¾ç½®ä¸­é—´ç»“æœä¿å­˜æ–‡ä»¶
        dataset, model = dataset_key.split('/')
        output_dir = self.eval_dir / dataset / model
        output_dir.mkdir(parents=True, exist_ok=True)
        tmp_file = output_dir / "parallel_tmp.pkl"
        
        # å¹¶è¡Œè¯„æµ‹æ‰€æœ‰é¢˜ç›®
        parallel_results = track_progress_rich(
            self._evaluate_single_problem,
            tasks,
            nproc=self.nproc,
            chunksize=max(1, self.nproc//2),
            keys=indices,
            save=str(tmp_file)
        )
        
        safe_print(f"âœ… å¹¶è¡Œè¯„æµ‹å®Œæˆï¼Œå¼€å§‹æ±‡æ€»ç»“æœ...")
        
        # æ±‡æ€»ç»“æœ
        results = self._aggregate_results(parallel_results, eval_data, dataset_key)
        
        # ä¿å­˜ç»“æœ
        self._save_evaluation_results(results, dataset_key, eval_data, parallel_results)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            if tmp_file.exists():
                tmp_file.unlink()
                safe_print(f"ğŸ—‘ï¸  æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {tmp_file}")
        except Exception as e:
            safe_print(f"âš ï¸  æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
        
        return results

    def _init_judge_model(self, judge_kwargs):
        """åˆå§‹åŒ–judgeæ¨¡å‹"""
        safe_print(f"ğŸ”§ å¼€å§‹åˆå§‹åŒ–Judgeæ¨¡å‹")
        
        judge_model_name = judge_kwargs.get('model', None)
        safe_print(f"   ğŸ¤– æŒ‡å®šçš„æ¨¡å‹åç§°: {judge_model_name}")
        
        if judge_model_name and judge_model_name != 'exact_matching':
            safe_print(f"   ğŸ”‘ æ£€æŸ¥APIå¯†é’¥...")
            if gpt_key_set():
                safe_print(f"   âœ… APIå¯†é’¥å·²è®¾ç½®")
                try:
                    model_kwargs = {
                        'model': judge_model_name,
                        'timeout': 300,
                        'retry': 2,
                        'temperature': 0.6,
                        'max_output_tokens': 16384,
                        'verbose': False,
                        **{k: v for k, v in judge_kwargs.items() if k not in ['model', 'nproc']}
                    }
                    test_model = build_judge(**model_kwargs)
                    if test_model.working():
                        safe_print(f"ğŸ¤– ä½¿ç”¨Judgeæ¨¡å‹: {judge_model_name}")
                        return test_model
                    else:
                        safe_print(f"   âŒ Judge APIä¸å·¥ä½œ")
                        warnings.warn('Judge APIä¸å·¥ä½œï¼Œè·³è¿‡è¿‡ç¨‹è¯„æµ‹')
                except Exception as e:
                    safe_print(f"   âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
                    warnings.warn(f'æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œè·³è¿‡è¿‡ç¨‹è¯„æµ‹')
            else:
                safe_print(f"   âŒ API_KEYæœªè®¾ç½®æˆ–æ— æ•ˆ")
                warnings.warn('API_KEYæ— æ•ˆï¼Œè·³è¿‡è¿‡ç¨‹è¯„æµ‹')
        else:
            safe_print("âš ï¸  æœªæŒ‡å®šJudgeæ¨¡å‹ï¼Œä»…è¿›è¡Œæœ€ç»ˆç­”æ¡ˆè¯„æµ‹")
        return None

    def _evaluate_single_problem(self, judge_model, row, index, judge_kwargs):
        """è¯„æµ‹å•ä¸ªé¢˜ç›®çš„å‡½æ•°ï¼ˆç”¨äºå¹¶è¡Œè°ƒç”¨ï¼‰"""
        task_id = f"é¢˜ç›®{index + 1}"
        log_buffer = LogBuffer(task_id)
        
        try:
            log_buffer.log(f"ğŸ“– å¼€å§‹è¯„æµ‹ - ID: {row.get('id', 'N/A')}")
            
            # æå–å­—æ®µ
            prediction = str(row['prediction']).strip()
            ground_truth = self._safe_parse_json_field(row.get('answer', ''))
            answer_type = self._safe_parse_json_field(row.get('answer_type', 'Open-End'))
            unit = self._safe_parse_json_field(row.get('unit', ''))
            # å°è¯•è¯»å–pointså­—æ®µï¼Œå¦‚æœä¸å­˜åœ¨åˆ™å°è¯•pointå­—æ®µï¼ˆå‘åå…¼å®¹ï¼‰
            points_value = row.get('points', row.get('point', 0))
            points = self._safe_parse_points_field(points_value)
            # è®°å½•ä½¿ç”¨çš„å­—æ®µåï¼ˆç”¨äºè°ƒè¯•ï¼‰
            points_field_used = 'points' if 'points' in row else ('point' if 'point' in row else 'default')
            marking = self._safe_parse_json_field(row.get('marking', ''))
            
            log_buffer.log(f"ğŸ“ é¢˜ç›®ä¿¡æ¯:")
            log_buffer.log(f"   - é¢„æµ‹ç­”æ¡ˆé•¿åº¦: {len(prediction)} å­—ç¬¦")
            log_buffer.log(f"   - æ ‡å‡†ç­”æ¡ˆ: {ground_truth}")
            log_buffer.log(f"   - åˆ†å€¼: {points} (å­—æ®µ: {points_field_used})")
            log_buffer.log(f"   - markingæ ‡å‡†æ•°é‡: {len(marking) if marking else 0}")
            
            item_total_points = sum(points) if points else 0.0
            log_buffer.log(f"   - æœ¬é¢˜æ€»åˆ†: {item_total_points}")
            
            # æ€»æ˜¯è¿›è¡Œç»†ç²’åº¦å’Œç²—ç²’åº¦è¯„æµ‹ï¼ˆå®Œå…¨å¯¹é½EuPhO2024é€»è¾‘ï¼‰
            # ç»†ç²’åº¦è¯„æµ‹
            log_buffer.log(f"ğŸ” å¼€å§‹ç»†ç²’åº¦è¯„æµ‹...")
            fine_grained_score, marking_detailed_scores = self._evaluate_fine_grained_with_buffer(
                prediction, marking, points, judge_model, row.get('question', ''), log_buffer
            )
            log_buffer.log(f"âœ… ç»†ç²’åº¦å¾—åˆ†: {fine_grained_score}")
            
            # ç²—ç²’åº¦è¯„æµ‹ï¼ˆä¼ å…¥fine_grained_scoreï¼Œå®Œå…¨å¯¹é½EuPhO2024é€»è¾‘ï¼‰
            log_buffer.log(f"ğŸ¯ å¼€å§‹ç²—ç²’åº¦è¯„æµ‹...")
            coarse_grained_score, extracted_pred = self._evaluate_coarse_grained_with_buffer(
                prediction, ground_truth, answer_type, unit, points, 
                fine_grained_score, row.get('question', ''), log_buffer
            )
            log_buffer.log(f"âœ… ç²—ç²’åº¦å¾—åˆ†: {coarse_grained_score}")
            log_buffer.log(f"ğŸ“¤ æå–çš„é¢„æµ‹ç­”æ¡ˆ: {extracted_pred}")
            
            # è®¡ç®—æœ€ç»ˆå¾—åˆ†ï¼ˆå–ä¸¤è€…æœ€å¤§å€¼ï¼‰
            final_score = max(fine_grained_score, coarse_grained_score)
            log_buffer.log(f"ğŸ“Š æœ€ç»ˆå¾—åˆ†: {final_score} (ç»†ç²’åº¦: {fine_grained_score}, ç²—ç²’åº¦: {coarse_grained_score})")
            
            result = {
                'index': index,
                'fine_grained_score': fine_grained_score,
                'coarse_grained_score': coarse_grained_score,
                'extracted_pred': extracted_pred,
                'marking_detailed_scores': marking_detailed_scores,
                'item_total_points': item_total_points,
                'ground_truth': ground_truth,
                'answer_type': answer_type,
                'unit': unit,
                'points': points,
                'marking': marking,
                'prediction': prediction,
                'earned_points': final_score  # æ·»åŠ earned_pointså­—æ®µï¼Œç­‰äºæœ€å¤§å¾—åˆ†
            }
            
            log_buffer.log(f"âœ… è¯„æµ‹å®Œæˆï¼Œæœ€ç»ˆå¾—åˆ†: {final_score}")
            log_buffer.flush()
            return result
            
        except Exception as e:
            log_buffer.log(f"âŒ è¯„æµ‹å¤±è´¥: {e}")
            import traceback
            log_buffer.log(f"ğŸ“„ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            log_buffer.flush()
            return None

    def _evaluate_fine_grained_with_buffer(self, prediction, marking, points, judge_model, question, log_buffer):
        """ç»†ç²’åº¦è¯„æµ‹ - å¸¦é‡æµ‹æœºåˆ¶ï¼ˆå¸¦æ—¥å¿—ç¼“å­˜ç‰ˆæœ¬ï¼‰"""
        log_buffer.log(f"   ğŸ” ç»†ç²’åº¦è¯„æµ‹å¼€å§‹")
        log_buffer.log(f"      - markingæ•°é‡: {len(marking) if marking else 0}")
        log_buffer.log(f"      - judge_model: {'æœ‰' if judge_model else 'æ— '}")
        
        if not marking or not judge_model:
            log_buffer.log(f"   âš ï¸  è·³è¿‡ç»†ç²’åº¦è¯„æµ‹ï¼š{'æ— markingæ ‡å‡†' if not marking else 'æ— judgeæ¨¡å‹'}")
            return 0.0, []
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¤šå¥—markingæ ‡å‡†ï¼ˆå¯¹é½EuPhO2024é€»è¾‘ï¼‰
        has_multiple_marking_sets = self._has_multiple_marking_sets(marking)
        if has_multiple_marking_sets:
            log_buffer.log(f"   ğŸ“‹ æ£€æµ‹åˆ°å¤šå¥—markingæ ‡å‡†ï¼Œå…± {len(marking)} å¥—")
            return self._evaluate_multiple_marking_sets_with_buffer(prediction, marking, points, judge_model, question, log_buffer)
        else:
            log_buffer.log(f"   ğŸ“‹ å•å¥—markingæ ‡å‡†")
            return self._evaluate_single_marking_set_with_buffer(prediction, marking, points, judge_model, question, log_buffer)
    
    def _has_multiple_marking_sets(self, marking):
        """æ£€æŸ¥æ˜¯å¦æœ‰å¤šå¥—markingæ ‡å‡†"""
        if not marking or len(marking) == 0:
            return False
        
        # å¦‚æœç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯åˆ—è¡¨ï¼Œåˆ™è®¤ä¸ºæœ‰å¤šå¥—æ ‡å‡†
        return isinstance(marking[0], list)
    
    def _evaluate_multiple_marking_sets_with_buffer(self, prediction, marking_sets, points, judge_model, question, log_buffer):
        """è¯„æµ‹å¤šå¥—markingæ ‡å‡†ï¼Œå–æœ€é«˜åˆ†"""
        best_score = 0.0
        best_detailed_scores = []
        all_marking_results = []
        
        max_possible_score = sum(points) if points else 0.0
        
        for set_idx, marking_set in enumerate(marking_sets):
            log_buffer.log(f"   ğŸ“Š è¯„æµ‹ç¬¬ {set_idx + 1} å¥—markingæ ‡å‡†")
            
            score, detailed_scores = self._evaluate_single_marking_set_with_buffer(
                prediction, marking_set, points, judge_model, question, log_buffer
            )
            
            # è®°å½•æ¯å¥—æ ‡å‡†çš„ç»“æœ
            marking_result = {
                'marking_set_index': set_idx + 1,
                'score': score,
                'detailed_scores': detailed_scores,
                'max_possible_score': max_possible_score
            }
            all_marking_results.append(marking_result)
            
            log_buffer.log(f"      âœ… ç¬¬ {set_idx + 1} å¥—æ ‡å‡†å¾—åˆ†: {score:.2f}")
            
            # æ›´æ–°æœ€ä½³åˆ†æ•°
            if score > best_score:
                best_score = score
                best_detailed_scores = detailed_scores
                # åœ¨æœ€ä½³è¯¦ç»†å¾—åˆ†ä¸­æ·»åŠ æ ‡è®°
                for detailed_score in best_detailed_scores:
                    detailed_score['best_marking_set'] = set_idx + 1
        
        log_buffer.log(f"   ğŸ† å¤šå¥—æ ‡å‡†æœ€ç»ˆå¾—åˆ†: {best_score:.2f} (æ¥è‡ªç¬¬ {[r['marking_set_index'] for r in all_marking_results if r['score'] == best_score][0]} å¥—æ ‡å‡†)")
        
        return round(best_score, 2), best_detailed_scores
    
    def _evaluate_single_marking_set_with_buffer(self, prediction, marking, points, judge_model, question, log_buffer):
        """è¯„æµ‹å•å¥—markingæ ‡å‡† - å¸¦é‡æµ‹æœºåˆ¶ï¼ˆå¸¦æ—¥å¿—ç¼“å­˜ç‰ˆæœ¬ï¼‰"""        
        scoring_criteria = self._parse_marking_criteria(marking)
        max_possible_score = sum(points) if points else 0.0
        max_retries = 3
        
        log_buffer.log(f"      ğŸ“Š è¯„æµ‹é…ç½®:")
        log_buffer.log(f"         - è¯„åˆ†æ ‡å‡†æ•°é‡: {len(scoring_criteria)}")
        log_buffer.log(f"         - æœ€å¤§æ€»åˆ†: {max_possible_score}")
        log_buffer.log(f"         - æœ€å¤§é‡æµ‹æ¬¡æ•°: {max_retries}")
        
        for attempt in range(max_retries + 1):
            log_buffer.log(f"      ğŸ”„ å¼€å§‹ç¬¬ {attempt + 1} æ¬¡è¯„æµ‹")
            scores = []
            detailed_scores = []
            
            # å¯¹æ¯ä¸ªmarkingæ ‡å‡†è¿›è¡Œè¯„åˆ†
            for i, criterion in enumerate(scoring_criteria):
                log_buffer.log(f"         ğŸ“ è¯„æµ‹æ ‡å‡† {i+1}/{len(scoring_criteria)}: {criterion['description'][:50]}{'...' if len(criterion['description']) > 50 else ''}")
                score, response = self._evaluate_single_criterion_with_buffer(
                    prediction, criterion, judge_model, question, 
                    max_total_score=max_possible_score, 
                    current_attempt=attempt,
                    log_buffer=log_buffer
                )
                scores.append(score)
                log_buffer.log(f"            â¡ï¸ å¾—åˆ†: {score}")
                
                # è®°å½•è¯¦ç»†å¾—åˆ†
                detailed_scores.append({
                    'marking_criterion': criterion['description'],
                    'score': round(score, 2),
                    'index': criterion['index'],
                    'attempt': attempt + 1,
                    'judge_response': response
                })
            
            total_score = sum(scores)
            log_buffer.log(f"      ğŸ“Š ç¬¬ {attempt + 1} æ¬¡è¯„æµ‹æ€»åˆ†: {total_score} (å„é¡¹å¾—åˆ†: {scores})")
            
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§åˆ†æ•°
            if total_score <= max_possible_score or max_possible_score == 0:
                # åˆ†æ•°åˆç†ï¼Œæ·»åŠ æˆåŠŸæ ‡è®°
                for detailed_score in detailed_scores:
                    detailed_score['retry_info'] = f"ç¬¬{attempt + 1}æ¬¡è¯„æµ‹æˆåŠŸ" if attempt > 0 else "é¦–æ¬¡è¯„æµ‹æˆåŠŸ"
                    detailed_score['total_attempts'] = attempt + 1
                    detailed_score['final_success'] = True
                
                if attempt > 0:
                    log_buffer.log(f"      âœ… ç¬¬{attempt + 1}æ¬¡è¯„æµ‹æˆåŠŸï¼Œæ€»åˆ† {total_score:.2f} <= {max_possible_score:.2f}")
                else:
                    log_buffer.log(f"      âœ… é¦–æ¬¡è¯„æµ‹æˆåŠŸï¼Œæ€»åˆ† {total_score:.2f} <= {max_possible_score:.2f}")
                
                return round(total_score, 2), detailed_scores
            else:
                # åˆ†æ•°è¶…é™ï¼Œå‡†å¤‡é‡æµ‹
                if attempt < max_retries:
                    log_buffer.log(f"      âš ï¸  ç¬¬{attempt + 1}æ¬¡è¯„æµ‹è¶…åˆ†: {total_score:.2f} > {max_possible_score:.2f}ï¼Œè¿›è¡Œç¬¬{attempt + 2}æ¬¡é‡æµ‹...")
                else:
                    # è¾¾åˆ°æœ€å¤§é‡æµ‹æ¬¡æ•°ï¼Œå¼ºåˆ¶è°ƒæ•´
                    log_buffer.log(f"      âŒ å·²è¾¾æœ€å¤§é‡æµ‹æ¬¡æ•°({max_retries + 1})ï¼Œæ€»åˆ†ä»è¶…é™: {total_score:.2f} > {max_possible_score:.2f}")
                    log_buffer.log(f"      ğŸ“Š å¼ºåˆ¶æŒ‰æ¯”ä¾‹è°ƒæ•´åˆ†æ•°...")
                    
                    scale_factor = max_possible_score / total_score
                    adjusted_scores = []
                    
                    log_buffer.log(f"         ğŸ“ è°ƒæ•´ç³»æ•°: {scale_factor:.3f}")
                    for i, score in enumerate(scores):
                        adjusted_score = score * scale_factor
                        adjusted_scores.append(adjusted_score)
                        log_buffer.log(f"            æ ‡å‡†{i+1}: {score:.2f} -> {adjusted_score:.2f}")
                        detailed_scores[i]['original_score'] = detailed_scores[i]['score']
                        detailed_scores[i]['score'] = round(adjusted_score, 2)
                        detailed_scores[i]['retry_info'] = f"é‡æµ‹{max_retries + 1}æ¬¡åå¼ºåˆ¶è°ƒæ•´"
                        detailed_scores[i]['total_attempts'] = max_retries + 1
                        detailed_scores[i]['forced_adjustment'] = True
                        detailed_scores[i]['scale_factor'] = round(scale_factor, 3)
                        detailed_scores[i]['final_success'] = False
                    
                    return round(sum(adjusted_scores), 2), detailed_scores
        
        return 0.0, []

    def _evaluate_coarse_grained_with_buffer(self, prediction, ground_truth, answer_type, unit, points, fine_grained_score, question, log_buffer):
        """ç²—ç²’åº¦è¯„æµ‹ï¼ˆå®Œå…¨å¯¹é½EuPhO2024é€»è¾‘ï¼‰"""
        log_buffer.log(f"   ğŸ¯ ç²—ç²’åº¦è¯„æµ‹å¼€å§‹")
        log_buffer.log(f"      - æ ‡å‡†ç­”æ¡ˆ: {ground_truth}")
        log_buffer.log(f"      - ç»†ç²’åº¦å¾—åˆ†: {fine_grained_score}")
        
        extracted_pred = ""
        
        if ground_truth:
            log_buffer.log(f"      âœ… æœ‰æ ‡å‡†ç­”æ¡ˆï¼Œå¼€å§‹ç­”æ¡ˆåŒ¹é…è¯„æµ‹")
            try:
                # æå–é¢„æµ‹ç­”æ¡ˆç”¨äºæ˜¾ç¤º
                num_expected_answers = len(ground_truth)
                log_buffer.log(f"      ğŸ“¤ æå–é¢„æµ‹ç­”æ¡ˆï¼ˆæœŸæœ›{num_expected_answers}ä¸ªç­”æ¡ˆï¼‰")
                extracted_pred = self._extract_prediction_for_display(prediction, num_expected_answers)
                log_buffer.log(f"      ğŸ“ æå–ç»“æœ: {extracted_pred}")
                
                # å¤šç­”æ¡ˆè¯„æµ‹
                log_buffer.log(f"      ğŸ” å¼€å§‹å¤šç­”æ¡ˆåŒ¹é…è¯„æµ‹")
                answer_score = self._evaluate_multiple_answers_with_buffer(prediction, ground_truth, points, question, log_buffer)
                log_buffer.log(f"      ğŸ“Š ç­”æ¡ˆåŒ¹é…å¾—åˆ†: {answer_score}")
                
                if answer_score > 0:
                    # ç­”æ¡ˆæ­£ç¡®ï¼Œä½¿ç”¨ç­”æ¡ˆå¾—åˆ†
                    log_buffer.log(f"      âœ… ç­”æ¡ˆæ­£ç¡®ï¼Œä½¿ç”¨ç­”æ¡ˆå¾—åˆ†: {answer_score}")
                    return round(answer_score, 2), extracted_pred
                else:
                    # ç­”æ¡ˆé”™è¯¯ï¼Œç›´æ¥ä½¿ç”¨å·²è®¡ç®—çš„ç»†ç²’åº¦å¾—åˆ†ï¼ˆé¿å…é‡å¤markingè¯„åˆ†ï¼‰
                    log_buffer.log(f"      âŒ ç­”æ¡ˆé”™è¯¯ï¼Œä½¿ç”¨ç»†ç²’åº¦å¾—åˆ†: {fine_grained_score}")
                    return round(fine_grained_score, 2), extracted_pred
            except Exception as e:
                # è¯„æµ‹å¤±è´¥ï¼Œä½¿ç”¨å·²è®¡ç®—çš„ç»†ç²’åº¦å¾—åˆ†
                log_buffer.log(f"      âš ï¸  ç­”æ¡ˆè¯„æµ‹å¤±è´¥: {e}ï¼Œä½¿ç”¨ç»†ç²’åº¦å¾—åˆ†: {fine_grained_score}")
                return round(fine_grained_score, 2), extracted_pred
        
        # å¦‚æœæ²¡æœ‰æ ‡å‡†ç­”æ¡ˆï¼Œå°è¯•æå–é¢„æµ‹ç­”æ¡ˆç”¨äºæ˜¾ç¤º
        log_buffer.log(f"      âš ï¸  æ— æ ‡å‡†ç­”æ¡ˆï¼Œå°è¯•æå–é¢„æµ‹ç­”æ¡ˆç”¨äºæ˜¾ç¤º")
        if not extracted_pred:
            try:
                extracted_pred = self._extract_prediction_for_display(prediction, 10)
                log_buffer.log(f"      ğŸ“ æå–çš„é¢„æµ‹ç­”æ¡ˆ: {extracted_pred}")
            except Exception as e:
                log_buffer.log(f"      âŒ æå–é¢„æµ‹ç­”æ¡ˆå¤±è´¥: {e}")
                extracted_pred = ""
        
        # æ²¡æœ‰æ ‡å‡†ç­”æ¡ˆæ—¶ï¼Œä½¿ç”¨ç»†ç²’åº¦å¾—åˆ†
        log_buffer.log(f"      ğŸ“Š æœ€ç»ˆä½¿ç”¨ç»†ç²’åº¦å¾—åˆ†: {fine_grained_score}")
        return round(fine_grained_score, 2), extracted_pred

    def _evaluate_coarse_grained_simple_with_buffer(self, prediction, ground_truth, answer_type, unit, points, question, log_buffer):
        """ç®€åŒ–çš„ç²—ç²’åº¦è¯„æµ‹ - ä»…è¿›è¡Œç­”æ¡ˆåŒ¹é…è¯„æµ‹ï¼ˆå¸¦æ—¥å¿—ç¼“å­˜ç‰ˆæœ¬ï¼‰"""
        log_buffer.log(f"   ğŸ¯ ç²—ç²’åº¦è¯„æµ‹å¼€å§‹")
        log_buffer.log(f"      - æ ‡å‡†ç­”æ¡ˆ: {ground_truth}")
        
        extracted_pred = ""
        
        if ground_truth:
            log_buffer.log(f"      âœ… æœ‰æ ‡å‡†ç­”æ¡ˆï¼Œå¼€å§‹ç­”æ¡ˆåŒ¹é…è¯„æµ‹")
            try:
                # æå–é¢„æµ‹ç­”æ¡ˆç”¨äºæ˜¾ç¤º
                num_expected_answers = len(ground_truth)
                log_buffer.log(f"      ğŸ“¤ æå–é¢„æµ‹ç­”æ¡ˆï¼ˆæœŸæœ›{num_expected_answers}ä¸ªç­”æ¡ˆï¼‰")
                extracted_pred = self._extract_prediction_for_display(prediction, num_expected_answers)
                log_buffer.log(f"      ğŸ“ æå–ç»“æœ: {extracted_pred}")
                
                # å¤šç­”æ¡ˆè¯„æµ‹
                log_buffer.log(f"      ğŸ” å¼€å§‹å¤šç­”æ¡ˆåŒ¹é…è¯„æµ‹")
                answer_score = self._evaluate_multiple_answers_with_buffer(prediction, ground_truth, points, question, log_buffer)
                log_buffer.log(f"      ğŸ“Š ç­”æ¡ˆåŒ¹é…å¾—åˆ†: {answer_score}")
                
                return round(answer_score, 2), extracted_pred
                
            except Exception as e:
                log_buffer.log(f"      âš ï¸  ç­”æ¡ˆè¯„æµ‹å¤±è´¥: {e}ï¼Œè¿”å›0åˆ†")
                return 0.0, extracted_pred
        
        # å¦‚æœæ²¡æœ‰æ ‡å‡†ç­”æ¡ˆï¼Œå°è¯•æå–é¢„æµ‹ç­”æ¡ˆç”¨äºæ˜¾ç¤º
        log_buffer.log(f"      âš ï¸  æ— æ ‡å‡†ç­”æ¡ˆï¼Œå°è¯•æå–é¢„æµ‹ç­”æ¡ˆç”¨äºæ˜¾ç¤º")
        if not extracted_pred:
            try:
                extracted_pred = self._extract_prediction_for_display(prediction, 10)
                log_buffer.log(f"      ğŸ“ æå–çš„é¢„æµ‹ç­”æ¡ˆ: {extracted_pred}")
            except Exception as e:
                log_buffer.log(f"      âŒ æå–é¢„æµ‹ç­”æ¡ˆå¤±è´¥: {e}")
                extracted_pred = ""
        
        log_buffer.log(f"      ğŸ“Š æ— æ ‡å‡†ç­”æ¡ˆï¼Œè¿”å›0åˆ†")
        return 0.0, extracted_pred

    def _evaluate_multiple_answers_with_buffer(self, prediction, ground_truth_list, points_list, question="", log_buffer=None):
        """å¤šç­”æ¡ˆè¯„æµ‹ï¼ˆå¸¦æ—¥å¿—ç¼“å­˜ç‰ˆæœ¬ï¼‰"""
        if not ground_truth_list:
            return 0.0
            
        # ç¡®ä¿æ•°æ®é•¿åº¦ä¸€è‡´
        actual_length = min(len(ground_truth_list), len(points_list))
        ground_truth_list = ground_truth_list[:actual_length]
        points_list = points_list[:actual_length]
        
        try:
            # ä½¿ç”¨physics_r1çš„å¤šç­”æ¡ˆè¯„æµ‹å‡½æ•°
            total_score, total_point, extracted_preds, extracted_gts, scored_by_list = answer_tag_reward_fn_for_r1(
                prediction, ground_truth_list, problem=question, points=points_list, 
                use_xverify=True, debug=True, log_callback=log_buffer.log if log_buffer else None
            )
            if log_buffer:
                log_buffer.log(f"         ğŸ“Š è¯„åˆ†è¯¦æƒ…: scored_by={scored_by_list}, total_point={total_point}")
            return total_point
        except Exception as e:
            if log_buffer:
                log_buffer.log(f"[DEBUG] Exception in answer evaluation: {str(e)}")
            # å›é€€åˆ°é€ä¸ªè¯„æµ‹
            return self._fallback_individual_evaluation_with_buffer(prediction, ground_truth_list, points_list, question, log_buffer)

    def _fallback_individual_evaluation_with_buffer(self, prediction, ground_truth_list, points_list, question="", log_buffer=None):
        """å›é€€çš„é€ä¸ªè¯„æµ‹æ–¹æ³•ï¼ˆå¸¦æ—¥å¿—ç¼“å­˜ç‰ˆæœ¬ï¼‰"""
        try:
            num_answers = len(ground_truth_list)
            extracted_answers = get_answer_str(prediction, return_origin=False, num_answers=num_answers)
            
            total_earned_score = 0.0
            for extracted_ans, gt_answer, points in zip(extracted_answers, ground_truth_list, points_list):
                if extracted_ans and extracted_ans.strip():
                    try:
                        is_correct, _, _, _ = grade(extracted_ans, gt_answer, False, problem=question, 
                                                 use_xverify=True, debug=True, 
                                                 log_callback=log_buffer.log if log_buffer else None)
                        if is_correct:
                            total_earned_score += points
                    except Exception as e:
                        if log_buffer:
                            log_buffer.log(f"[DEBUG] Exception in grade call: {str(e)}")
                        pass
            
            return total_earned_score
        except Exception as e:
            if log_buffer:
                log_buffer.log(f"[DEBUG] Exception in fallback evaluation: {str(e)}")
            return 0.0

    def _parse_marking_criteria(self, marking_list):
        """è§£æmarkingè¯„åˆ†æ ‡å‡†"""
        criteria = []
        if not marking_list:
            return criteria
        
        for i, marking_criterion in enumerate(marking_list):
            if marking_criterion and str(marking_criterion).strip():
                criteria.append({
                    'description': str(marking_criterion).strip(),
                    'index': i
                })
        
        return criteria

    def _evaluate_single_criterion_with_buffer(self, prediction, criterion, judge_model, question, max_total_score=None, current_attempt=0, log_buffer=None):
        """ä½¿ç”¨judgeæ¨¡å‹è¯„æµ‹å•ä¸ªæ ‡å‡† - å¸¦é‡æµ‹æœºåˆ¶ï¼ˆå¸¦æ—¥å¿—ç¼“å­˜ç‰ˆæœ¬ï¼‰"""
        log_buffer.log(f"         ğŸ¤– è°ƒç”¨Judgeæ¨¡å‹è¯„æµ‹æ ‡å‡†")
        
        # æ„å»ºæ€»åˆ†é™åˆ¶æç¤º
        total_score_warning = ""
        if max_total_score is not None and max_total_score > 0:
            total_score_warning = f"""
âš ï¸  IMPORTANT TOTAL SCORE CONSTRAINT:
- This question has a maximum total score of {max_total_score} points
- ALL marking criteria scores combined MUST NOT exceed {max_total_score} points
- You are evaluating ONE criterion among multiple criteria for this question
- Be conservative in your scoring to ensure the total doesn't exceed the limit
- This is attempt #{current_attempt + 1} of evaluation"""

        prompt = f"""You are an expert physics competition grader. Evaluate the student's solution against the specific grading criterion.

PHYSICS PROBLEM:
{question}

STUDENT'S SOLUTION:
{prediction}

GRADING CRITERION:
{criterion['description']}{total_score_warning}

INSTRUCTIONS:
1. Carefully analyze the student's solution for physics concepts, mathematical derivations, and calculations.
2. Compare the solution against the specific grading criterion provided.
3. Award points strictly according to the criterion, including partial credit when specified.
4. BE CONSERVATIVE - remember this is one of multiple criteria being evaluated simultaneously.

SCORING FORMAT:
- Read the grading criterion carefully to understand the maximum points and conditions for partial credit
- Evaluate whether the student's solution meets the full criteria, partial criteria, or no criteria
- Output your score using the exact format: \\boxed{{score}}
- The score should be a number (e.g., 0.4, 0.2, 0.1, 0.0)

CRITICAL REQUIREMENTS:
- You MUST output your final score in the format: \\boxed{{score}}
- The score must be a single number only (no text inside the boxed)
- BE CONSERVATIVE to avoid exceeding the total score limit

âš ï¸ CRITICAL INSTRUCTION: 
- Output ONLY: \\boxed{{score}}
- NO explanations, NO analysis, NO reasoning
- Just the number in the exact format \\boxed{{score}}

RESPOND WITH ONLY THE BOXED SCORE:"""
        
        try:
            log_buffer.log(f"         â³ è°ƒç”¨Judgeæ¨¡å‹...")
            start_time = time.time()
            
            response = judge_model.generate(prompt).strip()
            
            elapsed_time = time.time() - start_time
            log_buffer.log(f"         â±ï¸  å“åº”è€—æ—¶: {elapsed_time:.2f}ç§’")
            
            # æå–åˆ†æ•°
            score = self._extract_score_from_response(response)
            log_buffer.log(f"         ğŸ” æå–çš„åˆ†æ•°: {score}")
            
            return score, response
            
        except Exception as e:
            elapsed_time = time.time() - start_time
            log_buffer.log(f"         âŒ Judgeæ¨¡å‹è°ƒç”¨å¤±è´¥ (è€—æ—¶ {elapsed_time:.2f}ç§’): {e}")
            return 0.0, f"Judgeæ¨¡å‹è°ƒç”¨å¤±è´¥: {str(e)}"

    def _extract_score_from_response(self, response):
        """ä»æ¨¡å‹å“åº”ä¸­æå–åˆ†æ•°çš„è¾…åŠ©å‡½æ•°"""
        if not response:
            return 0.0
            
        response = response.strip()
        
        # ä¼˜å…ˆä½¿ç”¨boxedæ ¼å¼æå–åˆ†æ•°
        boxed_patterns = [
            r'\\boxed\{([^}]+)\}',
            r'boxed\{([^}]+)\}',
        ]
        
        for pattern in boxed_patterns:
            matches = re.findall(pattern, response, re.IGNORECASE)
            for match in reversed(matches):
                match = match.strip()
                if match:
                    try:
                        score = float(match)
                        return round(score, 2)
                    except ValueError:
                        nums = re.findall(r'\d+\.?\d*', match)
                        if nums:
                            try:
                                score = float(nums[-1])
                                return round(score, 2)
                            except ValueError:
                                continue
        
        # æŸ¥æ‰¾ç‰¹å®šæ ¼å¼çš„åˆ†æ•°
        score_patterns = [
            r'(?:Score|Final Score|Total|Points?):\s*([0-9]*\.?[0-9]+)',
            r'([0-9]*\.?[0-9]+)\s*(?:points?|pts?)',
        ]
        
        for pattern in score_patterns:
            matches = re.findall(pattern, response, re.IGNORECASE)
            if matches:
                try:
                    score = float(matches[-1])
                    return round(score, 2)
                except ValueError:
                    continue
        
        # æå–æ‰€æœ‰æ•°å­—ï¼Œå–æœ€åä¸€ä¸ª
        all_numbers = re.findall(r'[0-9]*\.?[0-9]+', response)
        if all_numbers:
            try:
                score = float(all_numbers[-1])
                return round(score, 2)
            except ValueError:
                pass
        
        return 0.0
    
    def _extract_prediction_for_display(self, prediction, num_answers=10):
        """æå–é¢„æµ‹ç­”æ¡ˆç”¨äºæ˜¾ç¤º"""
        try:
            extracted_answers = get_answer_str(prediction, return_origin=False, num_answers=num_answers)
            valid_answers = []
            
            for ans in extracted_answers:
                if ans and ans.strip():
                    cleaned_ans = ' '.join(ans.strip().replace('\n', ' ').replace('\r', ' ').split())
                    if cleaned_ans:
                        valid_answers.append(cleaned_ans)
            
            return ", ".join(valid_answers) if valid_answers else ""
        except Exception:
            # å›é€€åˆ°extract_boxed_answer
            try:
                extracted = extract_boxed_answer(prediction)
                if extracted and extracted.strip():
                    cleaned = ' '.join(extracted.strip().replace('\n', ' ').replace('\r', ' ').split())
                    return cleaned if cleaned else ""
            except Exception:
                pass
            return ""

    def _aggregate_results(self, parallel_results, eval_data, dataset_key):
        """æ±‡æ€»å¹¶è¡Œè¯„æµ‹ç»“æœï¼ˆå¯¹é½EuPhO2024é€»è¾‘ï¼‰"""
        fine_grained_total_score = 0.0
        coarse_grained_total_score = 0.0
        total_score = 0.0  # ä½¿ç”¨earned_pointsï¼ˆæœ€å¤§å€¼ï¼‰ä½œä¸ºæ€»åˆ†
        max_possible_score = 0.0
        
        for i, result in enumerate(parallel_results):
            if result is None:
                safe_print(f"âš ï¸  é¢˜ç›® {i+1} è¯„æµ‹å¤±è´¥ï¼Œè·³è¿‡")
                continue
                
            fine_score = result['fine_grained_score']
            coarse_score = result['coarse_grained_score']
            earned_points = result.get('earned_points', max(fine_score, coarse_score))
            item_points = result['item_total_points']
            
            # ç´¯åŠ å„ç§å¾—åˆ†ï¼ˆå¯¹é½EuPhO2024é€»è¾‘ï¼‰
            fine_grained_total_score = round(fine_grained_total_score + fine_score, 2)
            coarse_grained_total_score = round(coarse_grained_total_score + coarse_score, 2)
            total_score = round(total_score + earned_points, 2)  # æ€»åˆ†ä½¿ç”¨earned_points
            
            max_possible_score += item_points
        
        # è®¡ç®—æœ€ç»ˆç»“æœ
        max_possible_score = round(max_possible_score, 2)
        fine_rate = round((fine_grained_total_score / max_possible_score * 100), 2) if max_possible_score > 0 else 0.0
        coarse_rate = round((coarse_grained_total_score / max_possible_score * 100), 2) if max_possible_score > 0 else 0.0
        total_rate = round((total_score / max_possible_score * 100), 2) if max_possible_score > 0 else 0.0
        
        return {
            'dataset_key': dataset_key,
            'fine_grained_total_score': fine_grained_total_score,
            'fine_grained_score_rate': fine_rate,
            'coarse_grained_total_score': coarse_grained_total_score,
            'coarse_grained_score_rate': coarse_rate,
            'total_score': total_score,  # è¿™æ˜¯earned_pointsçš„æ€»å’Œ
            'score_rate': total_rate,
            'max_possible_score': max_possible_score,
            'total_count': len(parallel_results),
        }

    def _save_evaluation_results(self, results, dataset_key, eval_data, parallel_results, file_name=None):
        """ä¿å­˜è¯„æµ‹ç»“æœ"""
        dataset, model = dataset_key.split('/')
        output_dir = self.eval_dir / dataset / model
        if file_name:
            output_dir = output_dir / file_name
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æ±‡æ€»ç»“æœ
        score_file = output_dir / f"{file_name}_score.json"
        dump(results, str(score_file))
        
        # æ„å»ºè¯¦ç»†ç»“æœ
        detailed_results = []
        for i, result in enumerate(parallel_results):
            if result is None:
                continue
            
            row = eval_data.iloc[i]
            # ä½¿ç”¨earned_pointså­—æ®µï¼Œå¦‚æœæ²¡æœ‰åˆ™è®¡ç®—æœ€å¤§å€¼ï¼ˆå¯¹é½EuPhO2024é€»è¾‘ï¼‰
            earned_points = result.get('earned_points', max(result['fine_grained_score'], result['coarse_grained_score']))
            
            detailed_item = {
                "id": str(row.get('id', f"{dataset_key}_{i+1}")),
                "context": str(row.get('context', '')).strip(),
                "question": str(row.get('question', '')).strip(),
                "solution": str(row.get('solution', '')).strip(),
                "marking": result['marking'] if result['marking'] else [],
                "marking_detailed_scores": result['marking_detailed_scores'] if result['marking_detailed_scores'] else [],
                "answer": [f"\\boxed{{{ans}}}" for ans in result['ground_truth']] if result['ground_truth'] else [''],
                "answer_type": result['answer_type'] if result['answer_type'] else ['Open-End'],
                "unit": result['unit'] if result['unit'] else [''],
                "points": result['points'] if result['points'] else [0.0],
                "modality": str(row.get('modality', 'text')).strip(),
                "field": str(row.get('field', '')).strip(),
                "subfield": str(row.get('subfield', '')).strip(),
                "source": dataset_key,
                "test_result": str(result['prediction']),
                "test_answer": [f"\\boxed{{{ans.strip()}}}" for ans in result['extracted_pred'].split(", ") if ans.strip()] if result['extracted_pred'] else [''],
                "fine_grained_score": result['fine_grained_score'],
                "coarse_grained_score": result['coarse_grained_score'],
                "earned_points": earned_points
            }
            detailed_results.append(detailed_item)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        detailed_file = output_dir / f"{file_name}_detailed_results.json"
        dump(detailed_results, str(detailed_file))
        
        # ä¿å­˜Excelæ ¼å¼ï¼ˆå¸¦è¯„æµ‹ç»“æœï¼‰
        try:
            eval_data_with_results = eval_data.copy()
            eval_data_with_results['fine_grained_score'] = [r['fine_grained_score'] for r in detailed_results]
            eval_data_with_results['coarse_grained_score'] = [r['coarse_grained_score'] for r in detailed_results]
            eval_data_with_results['earned_points'] = [r['earned_points'] for r in detailed_results]
            eval_data_with_results['extracted_prediction'] = [", ".join(r['test_answer']).replace("\\boxed{", "").replace("}", "") for r in detailed_results]
            # å°†markingè¯¦ç»†å¾—åˆ†è½¬æ¢ä¸ºå¯è¯»å­—ç¬¦ä¸²æ ¼å¼ä¿å­˜åˆ°Excel
            eval_data_with_results['marking_detailed_scores'] = [
                json.dumps(r['marking_detailed_scores'], ensure_ascii=False) if r['marking_detailed_scores'] else '[]' 
                for r in detailed_results
            ]
            
            detailed_xlsx_file = output_dir / f"{file_name}_detailed.xlsx"
            dump(eval_data_with_results, str(detailed_xlsx_file))
        except Exception as e:
            safe_print(f"âš ï¸  ä¿å­˜è¯¦ç»†Excelæ–‡ä»¶å¤±è´¥: {e}")
        
        safe_print(f"ğŸ’¾ è¯„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")

    def evaluate_all_datasets(self, judge_kwargs: Optional[Dict] = None) -> Dict[str, Dict]:
        """è¯„æµ‹æ‰€æœ‰å¯ç”¨çš„æ•°æ®é›†"""
        available_datasets = self.detect_available_datasets()
        
        if not available_datasets:
            safe_print("âŒ æœªå‘ç°ä»»ä½•å¯ç”¨çš„æ•°æ®é›†")
            return {}
        
        safe_print(f"ğŸ¯ å¼€å§‹è¯„æµ‹ {len(available_datasets)} ä¸ªæ•°æ®é›†...")
        
        all_results = {}
        for dataset_key in available_datasets:
            safe_print(f"\n{'='*60}")
            safe_print(f"ğŸ“Š æ­£åœ¨è¯„æµ‹: {dataset_key}")
            safe_print(f"{'='*60}")
            
            try:
                results = self.evaluate_dataset(dataset_key, judge_kwargs)
                all_results[dataset_key] = results
                
                # æ‰“å°å•ä¸ªæ•°æ®é›†çš„æ€»ç»“
                safe_print(f"\nâœ… {dataset_key} è¯„æµ‹å®Œæˆï¼")
                safe_print(f"ğŸ† æ€»ä½“å¾—åˆ†: {results['total_score']:.2f} / {results['max_possible_score']:.2f} ({results['score_rate']:.2f}%)")
                safe_print(f"ğŸ“Š ç»†ç²’åº¦æ€»åˆ†: {results['fine_grained_total_score']:.2f} ({results['fine_grained_score_rate']:.2f}%)")
                safe_print(f"ğŸ¯ ç²—ç²’åº¦æ€»åˆ†: {results['coarse_grained_total_score']:.2f} ({results['coarse_grained_score_rate']:.2f}%)")
                safe_print(f"ğŸ“ˆ è¯„æµ‹é¢˜ç›®æ•°: {results['total_count']}")
                
            except Exception as e:
                safe_print(f"âŒ è¯„æµ‹ {dataset_key} å¤±è´¥: {e}")
                import traceback
                safe_print(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                all_results[dataset_key] = None
        
        # ä¿å­˜æ±‡æ€»ç»“æœ
        self._save_summary_results(all_results)
        
        return all_results

    def evaluate_multiple_runs(self, dataset_key: str, judge_kwargs: Optional[Dict] = None) -> Dict:
        """è¯„æµ‹å¤šæ¬¡è¿è¡Œç»“æœå¹¶è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        if judge_kwargs is None:
            judge_kwargs = {}
        
        safe_print(f"ğŸš€ å¼€å§‹è¯„æµ‹å¤šæ¬¡è¿è¡Œç»“æœ: {dataset_key}")
        
        # åŠ è½½æ‰€æœ‰è¿è¡Œçš„æ¨ç†ç»“æœ
        all_runs_results = self.load_multiple_runs_results(dataset_key)
        
        # å¯¹æ¯æ¬¡è¿è¡Œåˆ†åˆ«è¿›è¡Œè¯„æµ‹
        run_evaluation_results = {}
        for run_file, inference_results in all_runs_results.items():
            safe_print(f"\nğŸ“ˆ è¯„æµ‹ {run_file}...")
            
            # è½¬æ¢ä¸ºDataFrameè¿›è¡Œè¯„æµ‹
            eval_data = pd.DataFrame(inference_results)
            
            # åˆå§‹åŒ–judgeæ¨¡å‹
            judge_model = self._init_judge_model(judge_kwargs)
            
            # æ„å»ºä»»åŠ¡åˆ—è¡¨
            tasks = []
            indices = []
            for i in range(len(eval_data)):
                row = eval_data.iloc[i]
                task_kwargs = judge_kwargs.copy()
                task = (judge_model, row, i, task_kwargs)
                tasks.append(task)
                indices.append(i)
            
            # è®¾ç½®ä¸­é—´ç»“æœä¿å­˜æ–‡ä»¶
            output_dir = Path(f"{self.eval_dir}/{dataset_key}")
            output_dir.mkdir(parents=True, exist_ok=True)
            tmp_file = output_dir / f"parallel_tmp.pkl"
            
            # å¹¶è¡Œè¯„æµ‹æ‰€æœ‰é¢˜ç›®
            parallel_results = track_progress_rich(
                self._evaluate_single_problem,
                tasks,
                nproc=self.nproc,
                chunksize=max(1, self.nproc//2),
                keys=indices,
                save=str(tmp_file)
            )
            
            # æ±‡æ€»å•æ¬¡è¿è¡Œç»“æœ
            run_results = self._aggregate_results(parallel_results, eval_data, run_file.split('/')[-1].replace('.json', ''))
            self._save_evaluation_results(run_results, dataset_key, eval_data, parallel_results, run_file.split('/')[-1].replace('.json', ''))
            run_evaluation_results[run_file] = {
                'results': run_results,
                'detailed_results': parallel_results,
                'eval_data': eval_data
            }
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                if tmp_file.exists():
                    tmp_file.unlink()
            except Exception:
                pass
            
            safe_print(f"   âœ… {run_file} è¯„æµ‹å®Œæˆ: {run_results['total_score']:.2f}/{run_results['max_possible_score']:.2f} ({run_results['score_rate']:.2f}%)")
        
        # è®¡ç®—å¤šæ¬¡è¿è¡Œçš„ç»Ÿè®¡ä¿¡æ¯
        multi_run_stats = self._calculate_multi_run_statistics(run_evaluation_results, dataset_key)
        
        # ä¿å­˜å¤šæ¬¡è¿è¡Œè¯„æµ‹ç»“æœ
        self._save_multi_run_results(multi_run_stats, dataset_key, run_evaluation_results)
        
        return multi_run_stats

    def _calculate_multi_run_statistics(self, run_evaluation_results: Dict, dataset_key: str) -> Dict:
        """è®¡ç®—å¤šæ¬¡è¿è¡Œçš„ç»Ÿè®¡ä¿¡æ¯"""
        safe_print(f"\nğŸ“Š è®¡ç®—å¤šæ¬¡è¿è¡Œç»Ÿè®¡ä¿¡æ¯...")
        
        run_files = list(run_evaluation_results.keys())
        num_runs = len(run_files)
        
        # è·å–ç¬¬ä¸€æ¬¡è¿è¡Œçš„é¢˜ç›®ä¿¡æ¯ä½œä¸ºåŸºå‡†
        first_run = list(run_evaluation_results.values())[0]
        first_eval_data = first_run['eval_data']
        num_questions = len(first_eval_data)
        
        # åˆå§‹åŒ–ç»Ÿè®¡æ•°æ®ç»“æ„
        question_stats = {}
        
        # ä¸ºæ¯é“é¢˜ç›®åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
        for i in range(num_questions):
            row = first_eval_data.iloc[i]
            question_id = str(row.get('id', f"{dataset_key}_{i+1}"))
            question_stats[question_id] = {
                'question_id': question_id,
                'question': str(row.get('question', '')).strip(),
                'context': str(row.get('context', '')).strip(),
                'answer': row.get('answer', []),
                'points': row.get('points', row.get('point', [0.0])),
                'max_points': sum(self._safe_parse_points_field(row.get('points', row.get('point', [0.0])))),
                'runs': {},
                'statistics': {}
            }
        
        # æ”¶é›†æ¯æ¬¡è¿è¡Œçš„ç»“æœ
        for run_file, run_data in run_evaluation_results.items():
            detailed_results = run_data['detailed_results']
            eval_data = run_data['eval_data']
            
            for i, result in enumerate(detailed_results):
                if result is None:
                    continue
                
                row = eval_data.iloc[i]
                question_id = str(row.get('id', f"{dataset_key}_{i+1}"))
                
                if question_id in question_stats:
                    # ç¡®å®šä½¿ç”¨çš„åˆ†æ•°ï¼ˆç»†ç²’åº¦ä¼˜å…ˆï¼‰
                    has_marking = result['marking'] and len(result['marking']) > 0 and self._has_valid_marking(result['marking'])
                    earned_score = max(result['fine_grained_score'], result['coarse_grained_score']) if has_marking else result['coarse_grained_score']
                    
                    question_stats[question_id]['runs'][run_file] = {
                        'fine_grained_score': result['fine_grained_score'],
                        'coarse_grained_score': result['coarse_grained_score'],
                        'earned_score': earned_score,
                        'prediction': result['prediction'],
                        'extracted_prediction': result['extracted_pred'],
                        'evaluation_method': 'fine_grained' if has_marking else 'coarse_grained'
                    }
        
        # è®¡ç®—æ¯é“é¢˜ç›®çš„ç»Ÿè®¡ä¿¡æ¯
        for question_id, question_data in question_stats.items():
            runs_data = question_data['runs']
            if not runs_data:
                continue
            
            scores = [run_data['earned_score'] for run_data in runs_data.values()]
            fine_scores = [run_data['fine_grained_score'] for run_data in runs_data.values()]
            coarse_scores = [run_data['coarse_grained_score'] for run_data in runs_data.values()]
            
            question_data['statistics'] = {
                'num_runs': len(scores),
                'mean_score': round(np.mean(scores), 2),
                'std_score': round(np.std(scores), 2),
                'min_score': round(np.min(scores), 2),
                'max_score': round(np.max(scores), 2),
                'mean_fine_score': round(np.mean(fine_scores), 2),
                'std_fine_score': round(np.std(fine_scores), 2),
                'min_fine_score': round(np.min(fine_scores), 2),
                'max_fine_score': round(np.max(fine_scores), 2),
                'mean_coarse_score': round(np.mean(coarse_scores), 2),
                'std_coarse_score': round(np.std(coarse_scores), 2),
                'min_coarse_score': round(np.min(coarse_scores), 2),
                'max_coarse_score': round(np.max(coarse_scores), 2),
                'score_rate': round(np.mean(scores) / question_data['max_points'] * 100, 2) if question_data['max_points'] > 0 else 0.0
            }
        
        # è®¡ç®—æ•´ä½“ç»Ÿè®¡ä¿¡æ¯
        overall_stats = self._calculate_overall_multi_run_stats(run_evaluation_results, question_stats)
        
        return {
            'dataset_key': dataset_key,
            'num_runs': num_runs,
            'run_files': run_files,
            'question_statistics': question_stats,
            'overall_statistics': overall_stats,
            'run_results': {run_file: data['results'] for run_file, data in run_evaluation_results.items()}
        }

    def _calculate_overall_multi_run_stats(self, run_evaluation_results: Dict, question_stats: Dict) -> Dict:
        """è®¡ç®—æ•´ä½“å¤šæ¬¡è¿è¡Œç»Ÿè®¡ä¿¡æ¯"""
        run_files = list(run_evaluation_results.keys())
        num_runs = len(run_files)
        
        # æ”¶é›†æ¯æ¬¡è¿è¡Œçš„æ€»ä½“å¾—åˆ†
        run_total_scores = []
        run_max_scores = []
        run_score_rates = []
        
        for run_file, run_data in run_evaluation_results.items():
            results = run_data['results']
            run_total_scores.append(results['total_score'])
            run_max_scores.append(results['max_possible_score'])
            run_score_rates.append(results['score_rate'])
        
        # è®¡ç®—æ¯é“é¢˜ç›®çš„å¹³å‡å¾—åˆ†
        question_mean_scores = []
        question_max_points = []
        
        for question_data in question_stats.values():
            if question_data['runs']:
                question_mean_scores.append(question_data['statistics']['mean_score'])
                question_max_points.append(question_data['max_points'])
        
        total_mean_score = sum(question_mean_scores)
        total_max_score = sum(question_max_points)
        
        return {
            'num_runs': num_runs,
            'num_questions': len(question_stats),
            'mean_total_score': round(np.mean(run_total_scores), 2),
            'std_total_score': round(np.std(run_total_scores), 2),
            'min_total_score': round(np.min(run_total_scores), 2),
            'max_total_score': round(np.max(run_total_scores), 2),
            'mean_score_rate': round(np.mean(run_score_rates), 2),
            'std_score_rate': round(np.std(run_score_rates), 2),
            'question_based_mean_score': round(total_mean_score, 2),
            'question_based_max_score': round(total_max_score, 2),
            'question_based_score_rate': round(total_mean_score / total_max_score * 100, 2) if total_max_score > 0 else 0.0
        }

    def _save_summary_results(self, all_results):
        """ä¿å­˜æ‰€æœ‰æ•°æ®é›†çš„æ±‡æ€»ç»“æœ"""
        # ä¿å­˜å®Œæ•´ç»“æœ
        summary_file = self.eval_dir / "all_datasets_summary.json"
        dump(all_results, str(summary_file))
        
        # åˆ›å»ºç®€åŒ–çš„æ±‡æ€»è¡¨
        summary_table = []
        for dataset_key, results in all_results.items():
            if results is None:
                continue
                
            summary_table.append({
                'dataset_key': dataset_key,
                'total_questions': results['total_count'],
                'total_score': results['total_score'],
                'max_possible_score': results['max_possible_score'],
                'score_rate': results['score_rate'],
                'total_count': results['total_count'],
                'fine_grained_score': results['fine_grained_total_score'],
                'fine_grained_rate': results['fine_grained_score_rate'],
                'coarse_grained_score': results['coarse_grained_total_score'],
                'coarse_grained_rate': results['coarse_grained_score_rate']
            })
        
        # ä¿å­˜æ±‡æ€»è¡¨
        summary_table_file = self.eval_dir / "summary_table.json"
        dump(summary_table, str(summary_table_file))
        
        # æ‰“å°æœ€ç»ˆæ±‡æ€»
        safe_print(f"\n{'='*80}")
        safe_print(f"ğŸ† æ‰€æœ‰æ•°æ®é›†è¯„æµ‹å®Œæˆï¼æ±‡æ€»ç»“æœ:")
        safe_print(f"{'='*80}")
        
        total_score_all = sum(r['total_score'] for r in all_results.values() if r)
        total_max_all = sum(r['max_possible_score'] for r in all_results.values() if r)
        overall_rate = round((total_score_all / total_max_all * 100), 2) if total_max_all > 0 else 0.0
        
        safe_print(f"ğŸ“Š æ•´ä½“è¡¨ç°: {total_score_all:.2f} / {total_max_all:.2f} ({overall_rate:.2f}%)")
        safe_print(f"ğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {self.eval_data}/")
        safe_print(f"ğŸ’¾ æ±‡æ€»æ–‡ä»¶: {summary_file}")
        safe_print(f"ğŸ“‹ æ±‡æ€»è¡¨æ ¼: {summary_table_file}")
        
        for item in summary_table:
            safe_print(f"   {item['dataset_key']}: {item['score_rate']:.2f}% ({item['total_score']:.1f}/{item['max_possible_score']:.1f})")

    def _save_multi_run_results(self, multi_run_stats: Dict, dataset_key: str, run_evaluation_results: Dict):
        """ä¿å­˜å¤šæ¬¡è¿è¡Œè¯„æµ‹ç»“æœ"""
        dataset, model = dataset_key.split('/')
        output_dir = self.eval_dir / dataset / model
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜å®Œæ•´çš„å¤šæ¬¡è¿è¡Œç»Ÿè®¡ç»“æœ
        stats_file = output_dir / f"multi_run_statistics.json"
        dump(multi_run_stats, str(stats_file))
        
        # ä¿å­˜æ¯é“é¢˜ç›®çš„è¯¦ç»†ç»Ÿè®¡ï¼ˆExcelæ ¼å¼ï¼‰
        question_stats_list = []
        fine_grained_question_stats_list = []
        coarse_grained_question_stats_list = []
        for question_id, question_data in multi_run_stats['question_statistics'].items():
            stats = question_data['statistics']
            if not stats:
                continue
            
            # æ”¶é›†æ¯æ¬¡è¿è¡Œçš„å¾—åˆ†
            run_scores = {}
            fine_grained_run_scores = {}
            coarse_grained_run_scores = {}
            for run_file in multi_run_stats['run_files']:
                if run_file in question_data['runs']:
                    run_scores[f"{run_file}_score"] = question_data['runs'][run_file]['earned_score']
                    fine_grained_run_scores[f"{run_file}_score"] = question_data['runs'][run_file]['fine_grained_score']
                    coarse_grained_run_scores[f"{run_file}_score"] = question_data['runs'][run_file]['coarse_grained_score']
                else:
                    run_scores[f"{run_file}_score"] = 0.0
                    fine_grained_run_scores[f"{run_file}_score"] = 0.0
                    coarse_grained_run_scores[f"{run_file}_score"] = 0.0
            
            question_stats_list.append({
                'question_id': question_id,
                'question': question_data['question'][:100] + '...' if len(question_data['question']) > 100 else question_data['question'],
                'max_points': question_data['max_points'],
                'mean_score': stats['mean_score'],
                'std_score': stats['std_score'],
                'min_score': stats['min_score'],
                'max_score': stats['max_score'],
                'score_rate': stats['score_rate'],
                'num_runs': stats['num_runs'],
                **run_scores
            })
            fine_grained_question_stats_list.append({
                'question_id': question_id,
                'question': question_data['question'][:100] + '...' if len(question_data['question']) > 100 else question_data['question'],
                'max_points': question_data['max_points'],
                'mean_score': stats['mean_fine_score'],
                'std_score': stats['std_fine_score'],
                'min_score': stats['min_fine_score'],
                'max_score': stats['max_fine_score'],
                'score_rate': stats['score_rate'],
                'num_runs': stats['num_runs'],
                **fine_grained_run_scores
            })
            coarse_grained_question_stats_list.append({
                'question_id': question_id,
                'question': question_data['question'][:100] + '...' if len(question_data['question']) > 100 else question_data['question'],
                'max_points': question_data['max_points'],
                'mean_score': stats['mean_coarse_score'],
                'std_score': stats['std_coarse_score'],
                'min_score': stats['min_coarse_score'],
                'max_score': stats['max_coarse_score'],
                'score_rate': stats['score_rate'],
                'num_runs': stats['num_runs'],
                **coarse_grained_run_scores
            })
        
        # ä¿å­˜é¢˜ç›®ç»Ÿè®¡Excel
        if question_stats_list:
            question_stats_df = pd.DataFrame(question_stats_list)
            question_stats_file = output_dir / f"question_statistics.xlsx"
            question_stats_df.to_excel(question_stats_file, index=False)
            safe_print(f"ğŸ“Š é¢˜ç›®ç»Ÿè®¡å·²ä¿å­˜: {question_stats_file}")

        if fine_grained_question_stats_list:
            fine_grained_question_stats_df = pd.DataFrame(fine_grained_question_stats_list)
            fine_grained_question_stats_file = output_dir / f"fine_grained_question_statistics.xlsx"
            fine_grained_question_stats_df.to_excel(fine_grained_question_stats_file, index=False)
            safe_print(f"ğŸ“Š ç»†ç²’åº¦é¢˜ç›®ç»Ÿè®¡å·²ä¿å­˜: {fine_grained_question_stats_file}")
            
        if coarse_grained_question_stats_list:
            coarse_grained_question_stats_df = pd.DataFrame(coarse_grained_question_stats_list)
            coarse_grained_question_stats_file = output_dir / f"coarse_grained_question_statistics.xlsx"
            coarse_grained_question_stats_df.to_excel(coarse_grained_question_stats_file, index=False)
            safe_print(f"ğŸ“Š ç²—ç²’åº¦é¢˜ç›®ç»Ÿè®¡å·²ä¿å­˜: {coarse_grained_question_stats_file}")
        
        # ä¿å­˜è¿è¡Œæ±‡æ€»ç»Ÿè®¡
        run_summary_list = []
        for run_file in multi_run_stats['run_files']:
            if run_file in run_evaluation_results:
                results = run_evaluation_results[run_file]['results']
                run_summary_list.append({
                    'run_id': run_file.split('/')[-1].replace('.json', ''),
                    'total_score': results['total_score'],
                    'max_possible_score': results['max_possible_score'],
                    'score_rate': results['score_rate'],
                    'total_count': results['total_count'],
                    'fine_grained_score': results['fine_grained_total_score'],
                    'fine_grained_rate': results['fine_grained_score_rate'],
                    'coarse_grained_score': results['coarse_grained_total_score'],
                    'coarse_grained_rate': results['coarse_grained_score_rate']
                })
        
        if run_summary_list:
            run_summary_df = pd.DataFrame(run_summary_list)
            run_summary_file = output_dir / f"run_summary.xlsx"
            run_summary_df.to_excel(run_summary_file, index=False)
            safe_print(f"ğŸ“ˆ è¿è¡Œæ±‡æ€»å·²ä¿å­˜: {run_summary_file}")
        
        # æ‰“å°å¤šæ¬¡è¿è¡Œæ±‡æ€»ä¿¡æ¯
        overall = multi_run_stats['overall_statistics']
        safe_print(f"\nğŸ† å¤šæ¬¡è¿è¡Œè¯„æµ‹å®Œæˆï¼")
        safe_print(f"ğŸ“Š æ•°æ®é›†: {multi_run_stats['dataset_key']}")
        safe_print(f"ğŸ”„ è¿è¡Œæ¬¡æ•°: {overall['num_runs']}")
        safe_print(f"ğŸ“ é¢˜ç›®æ•°é‡: {overall['num_questions']}")
        safe_print(f"ğŸ“ˆ å¹³å‡æ€»åˆ†: {overall['mean_total_score']:.2f} Â± {overall['std_total_score']:.2f}")
        safe_print(f"ğŸ¯ å¹³å‡å¾—åˆ†ç‡: {overall['mean_score_rate']:.2f}% Â± {overall['std_score_rate']:.2f}%")
        safe_print(f"ğŸ“‹ åŸºäºé¢˜ç›®å¹³å‡: {overall['question_based_mean_score']:.2f}/{overall['question_based_max_score']:.2f} ({overall['question_based_score_rate']:.2f}%)")
        safe_print(f"ğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
        
        return multi_run_stats

    def _save_all_multi_run_summary(self, all_multi_run_results: Dict):
        """ä¿å­˜æ‰€æœ‰æ•°æ®é›†å¤šæ¬¡è¿è¡Œç»“æœçš„æ±‡æ€»"""
        # ä¿å­˜å®Œæ•´ç»“æœ
        summary_file = self.eval_dir / "all_datasets_multi_run_summary.json"
        dump(all_multi_run_results, str(summary_file))
        
        # åˆ›å»ºæ±‡æ€»è¡¨æ ¼
        summary_table = []
        for dataset_key, multi_run_results in all_multi_run_results.items():
            if multi_run_results is None:
                continue
            
            overall = multi_run_results['overall_statistics']
            
            summary_table.append({
                'dataset_key': dataset_key,
                'num_runs': overall['num_runs'],
                'num_questions': overall['num_questions'],
                'mean_total_score': overall['mean_total_score'],
                'std_total_score': overall['std_total_score'],
                'mean_score_rate': overall['mean_score_rate'],
                'std_score_rate': overall['std_score_rate'],
                'question_based_mean_score': overall['question_based_mean_score'],
                'question_based_max_score': overall['question_based_max_score'],
                'question_based_score_rate': overall['question_based_score_rate']
            })
        
        # ä¿å­˜æ±‡æ€»è¡¨æ ¼
        if summary_table:
            summary_table_df = pd.DataFrame(summary_table)
            summary_table_file = self.eval_dir / "multi_run_summary_table.xlsx"
            summary_table_df.to_excel(summary_table_file, index=False)
            safe_print(f"ğŸ“‹ å¤šæ¬¡è¿è¡Œæ±‡æ€»è¡¨å·²ä¿å­˜: {summary_table_file}")
        
        # æ‰“å°æœ€ç»ˆæ±‡æ€»
        safe_print(f"\n{'='*80}")
        safe_print(f"ğŸ† æ‰€æœ‰æ•°æ®é›†å¤šæ¬¡è¿è¡Œè¯„æµ‹å®Œæˆï¼æ±‡æ€»ç»“æœ:")
        safe_print(f"{'='*80}")
        
        for item in summary_table:
            safe_print(f"   {item['dataset_key']}: {item['mean_score_rate']:.2f}% Â± {item['std_score_rate']:.2f}% ({item['num_runs']} runs)")
        
        safe_print(f"ğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {self.eval_dir}/")
        safe_print(f"ğŸ’¾ æ±‡æ€»æ–‡ä»¶: {summary_file}")
        safe_print(f"ğŸ“‹ æ±‡æ€»è¡¨æ ¼: {summary_table_file}")

    def fix_existing_statistics(self, base_results_dir=None):
        """ä¿®å¤å·²æœ‰çš„å¤šæ¬¡è¿è¡Œç»Ÿè®¡ç»“æœä¸­çš„earned_scoreé”™è¯¯"""
        if base_results_dir is None:
            base_results_dir = "evaluation_results"
        
        base_path = Path(base_results_dir)
        fixed_count = 0
        
        # æŸ¥æ‰¾æ‰€æœ‰å¤šæ¬¡è¿è¡Œç»Ÿè®¡æ–‡ä»¶
        stat_files = list(base_path.glob("**/*multi_run_statistics.json"))
        
        safe_print("Found {} statistics files to fix".format(len(stat_files)))
        
        for stat_file in stat_files:
            # try:
            safe_print("Fixing file: {}".format(stat_file))
            
            # è¯»å–åŸå§‹ç»Ÿè®¡æ•°æ®
            with open(stat_file, 'r', encoding='utf-8') as f:
                stats = json.load(f)
            
            modified = False
            
            # ä¿®å¤æ¯é“é¢˜çš„æ¯æ¬¡è¿è¡Œçš„earned_score
            if 'question_statistics' in stats:
                for question_id, question_data in stats['question_statistics'].items():
                    if 'runs' in question_data:
                        for run_id, run_data in question_data['runs'].items():
                            # é‡æ–°è®¡ç®—earned_scoreä¸ºæœ€å¤§å€¼
                            fine_score = run_data.get('fine_grained_score', 0)
                            coarse_score = run_data.get('coarse_grained_score', 0)
                            correct_earned_score = max(fine_score, coarse_score)
                            
                            if run_data.get('earned_score') != correct_earned_score:
                                safe_print("    {} {}: {} -> {}".format(question_id, run_id, run_data.get('earned_score'), correct_earned_score))
                                run_data['earned_score'] = correct_earned_score
                                modified = True
            
            # æ€»æ˜¯é‡æ–°è®¡ç®—æ±‡æ€»ç»Ÿè®¡ï¼ˆç¡®ä¿ç»Ÿè®¡æ•°æ®ä¸earned_scoreä¸€è‡´ï¼‰
            if 'question_statistics' in stats:
                stats_updated = False
                for question_id, question_data in stats['question_statistics'].items():
                    if 'runs' in question_data:
                        runs_data = question_data['runs']
                        if runs_data:
                            # é‡æ–°è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
                            scores = [run_data['earned_score'] for run_data in runs_data.values()]
                            fine_scores = [run_data['fine_grained_score'] for run_data in runs_data.values()]
                            coarse_scores = [run_data['coarse_grained_score'] for run_data in runs_data.values()]
                            
                            new_mean = round(np.mean(scores), 4)
                            new_std = round(np.std(scores), 4)
                            new_min = round(min(scores), 4)
                            new_max = round(max(scores), 4)
                            new_median = round(np.median(scores), 4)
                            new_mean_fine = round(np.mean(fine_scores), 4)
                            new_mean_coarse = round(np.mean(coarse_scores), 4)
                            
                            # è®¡ç®—score_rate
                            max_points = question_data.get('max_points', 1)
                            new_score_rate = round((new_mean / max_points * 100) if max_points > 0 else 0, 2)
                            
                            # æ›´æ–°statisticså­å­—æ®µ
                            if 'statistics' not in question_data:
                                question_data['statistics'] = {}
                            
                            question_data['statistics']['mean_score'] = new_mean
                            question_data['statistics']['std_score'] = new_std
                            question_data['statistics']['min_score'] = new_min
                            question_data['statistics']['max_score'] = new_max
                            question_data['statistics']['median_score'] = new_median
                            question_data['statistics']['score_rate'] = new_score_rate
                            question_data['statistics']['mean_fine_score'] = new_mean_fine
                            question_data['statistics']['std_fine_score'] = new_std_fine
                            question_data['statistics']['min_fine_score'] = new_min_fine
                            question_data['statistics']['max_fine_score'] = new_max_fine
                            question_data['statistics']['median_fine_score'] = new_median_fine
                            question_data['statistics']['mean_coarse_score'] = new_mean_coarse
                            question_data['statistics']['std_coarse_score'] = new_std_coarse
                            question_data['statistics']['min_coarse_score'] = new_min_coarse
                            question_data['statistics']['max_coarse_score'] = new_max_coarse
                            question_data['statistics']['median_coarse_score'] = new_median_coarse
                            question_data['statistics']['num_runs'] = len(scores)
                            
                            # ä¹Ÿæ›´æ–°é¡¶çº§å­—æ®µï¼ˆå‘åå…¼å®¹ï¼‰
                            question_data['mean_score'] = new_mean
                            question_data['std_score'] = new_std
                            question_data['min_score'] = new_min
                            question_data['max_score'] = new_max
                            question_data['median_score'] = new_median
                            
                            stats_updated = True
                
                # å¦‚æœç»Ÿè®¡æ•°æ®è¢«æ›´æ–°äº†ï¼Œæ ‡è®°ä¸ºéœ€è¦ä¿å­˜
                if stats_updated and not modified:
                    modified = True
                    safe_print("    Statistics recalculated")
            
            # é‡æ–°è®¡ç®—æ•´ä½“ç»Ÿè®¡
            if 'question_statistics' in stats and modified:
                all_scores = []
                for question_data in stats['question_statistics'].values():
                    if 'runs' in question_data:
                        for run_data in question_data['runs'].values():
                            all_scores.append(run_data['earned_score'])
                
                if all_scores:
                    stats['overall_statistics'] = {
                        'total_runs': len(all_scores),
                        'mean_score': round(np.mean(all_scores), 4),
                        'std_score': round(np.std(all_scores), 4),
                        'min_score': round(min(all_scores), 4),
                        'max_score': round(max(all_scores), 4),
                        'median_score': round(np.median(all_scores), 4)
                    }
            
            # ä¿å­˜ä¿®å¤åçš„æ–‡ä»¶å’Œé‡æ–°ç”ŸæˆExcel
            if modified:
                # å¤‡ä»½åŸæ–‡ä»¶
                backup_file = stat_file.with_suffix('.json.backup')
                import shutil
                shutil.copy2(stat_file, backup_file)
                
                # ä¿å­˜ä¿®å¤åçš„æ–‡ä»¶
                with open(stat_file, 'w', encoding='utf-8') as f:
                    json.dump(stats, f, ensure_ascii=False, indent=4)
                
                safe_print("    Fixed, backup saved as: {}".format(backup_file.name))
                fixed_count += 1
            else:
                safe_print("    No fix needed")
            
            # æ€»æ˜¯é‡æ–°ç”ŸæˆExcelæ–‡ä»¶ï¼ˆç¡®ä¿Excelä¸JSONä¸€è‡´ï¼‰
            # try:
            self._regenerate_excel_files(stat_file.parent, stats)
            safe_print("    Excel files regenerated")
            # except Exception as e:
            #     safe_print("    Excel regeneration failed: {}".format(e))
                    
            # except Exception as e:
            #     safe_print("    Fix failed: {}".format(e))
        
        # é‡æ–°ç”Ÿæˆæ€»æ±‡æ€»è¡¨æ ¼
        if fixed_count > 0:
            self._regenerate_summary_table(base_path)
        
        safe_print("Fix completed! Fixed {} statistics files".format(fixed_count))
        return fixed_count

    def _regenerate_excel_files(self, output_dir, multi_run_stats):
        """é‡æ–°ç”ŸæˆExcelç»Ÿè®¡æ–‡ä»¶"""
        # try:
        import shutil
        # æå–æ–‡ä»¶å‰ç¼€ï¼ˆä»æ•°æ®é›†åç§°è·å–ï¼Œä¸ä¾èµ–ç›®å½•åï¼‰
        file_prefix = multi_run_stats.get('dataset_key', 'dataset')
        print(f"{file_prefix=}")
        if not file_prefix or file_prefix == 'dataset':
            # å¦‚æœæ²¡æœ‰dataset_keyï¼Œä»ç›®å½•åæ¨æ–­æ•°æ®é›†åç§°
            dir_name = output_dir.name
            if "_multiple_runs" in dir_name:
                # ä»ç›®å½•åä¸­æå–æ•°æ®é›†åç§°ï¼ˆå»æ‰æ¨¡å‹å‰ç¼€ï¼‰
                temp_name = dir_name.replace("_multiple_runs", "")
                # æŸ¥æ‰¾å·²çŸ¥çš„æ•°æ®é›†åç§°
                known_datasets = ['apho_2025', 'apho_2024', 'ipho_2025', 'ipho_2024', 
                                'eupho_2025', 'eupho_2024', 'nbpho_2025', 'nbpho_2024',
                                'panpho_2025', 'panpho_2024', 'panpho_mechanics_2025', 'panpho_mechanics_2024',
                                'fma_2025', 'fma_2024']
                for dataset in known_datasets:
                    if dataset in temp_name:
                        file_prefix = dataset
                        break
                else:
                    file_prefix = temp_name
            else:
                file_prefix = dir_name
        
        # é‡æ–°ç”Ÿæˆé¢˜ç›®ç»Ÿè®¡Excel
        question_stats_list = []
        fine_grained_question_stats_list = []
        coarse_grained_question_stats_list = []
        if 'question_statistics' in multi_run_stats:
            for question_id, question_data in multi_run_stats['question_statistics'].items():
                # æ”¶é›†æ¯æ¬¡è¿è¡Œçš„å¾—åˆ†ï¼ˆä½¿ç”¨ä¿®å¤åçš„earned_scoreï¼‰
                run_scores = {}
                fine_grained_run_scores = {}
                coarse_grained_run_scores = {}
                for run_dir in multi_run_stats.get('run_files', []):
                    if run_dir in question_data.get('runs', {}):
                        run_scores["{}_score".format(run_dir)] = question_data['runs'][run_dir]['earned_score']
                        fine_grained_run_scores["{}_score".format(run_dir)] = question_data['runs'][run_dir]['fine_grained_score']
                        coarse_grained_run_scores["{}_score".format(run_dir)] = question_data['runs'][run_dir]['coarse_grained_score']
                    else:
                        run_scores["{}_score".format(run_dir)] = 0.0
                        fine_grained_run_scores["{}_score".format(run_dir)] = 0.0
                        coarse_grained_run_scores["{}_score".format(run_dir)] = 0.0
                
                question_text = question_data.get('question', '')
                if len(question_text) > 100:
                    question_text = question_text[:100] + '...'
                
                # ä»statisticså­å­—æ®µè·å–ç»Ÿè®¡ä¿¡æ¯ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä»question_dataç›´æ¥è·å–
                stats = question_data.get('statistics', question_data)
                
                question_stats_list.append({
                    'question_id': question_id,
                    'question': question_text,
                    'max_points': question_data.get('max_points', 0),
                    'mean_score': stats.get('mean_score', 0),
                    'std_score': stats.get('std_score', 0),
                    'min_score': stats.get('min_score', 0),
                    'max_score': stats.get('max_score', 0),
                    'score_rate': stats.get('score_rate', 0),
                    'num_runs': stats.get('num_runs', 0),
                    **run_scores
                })
                fine_grained_question_stats_list.append({
                    'question_id': question_id,
                    'question': question_text,
                    'max_points': question_data.get('max_points', 0),
                    'mean_score': stats.get('mean_fine_score', 0),
                    'std_score': stats.get('std_fine_score', 0),
                    'min_score': stats.get('min_fine_score', 0),
                    'max_score': stats.get('max_fine_score', 0),
                    'score_rate': stats.get('score_rate', 0),
                    'num_runs': stats.get('num_runs', 0),
                    **fine_grained_run_scores
                })
                coarse_grained_question_stats_list.append({
                    'question_id': question_id,
                    'question': question_text,
                    'max_points': question_data.get('max_points', 0),
                    'mean_score': stats.get('mean_coarse_score', 0),
                    'std_score': stats.get('std_coarse_score', 0),
                    'min_score': stats.get('min_coarse_score', 0),
                    'max_score': stats.get('max_coarse_score', 0),
                    'score_rate': stats.get('score_rate', 0),
                    'num_runs': stats.get('num_runs', 0),
                    **coarse_grained_run_scores
                })
        
        if question_stats_list:
            question_stats_df = pd.DataFrame(question_stats_list)
            question_stats_file = output_dir / "question_statistics.xlsx"
            print(f"{question_stats_file=}")
            # å¤‡ä»½åŸExcelæ–‡ä»¶
            if question_stats_file.exists():
                backup_excel = question_stats_file.with_suffix('.xlsx.backup')
                shutil.copy2(question_stats_file, backup_excel)
            
            question_stats_df.to_excel(question_stats_file, index=False)
            safe_print("    Regenerated question statistics Excel: {}".format(question_stats_file.name))
        
        if fine_grained_question_stats_list:
            fine_grained_question_stats_df = pd.DataFrame(fine_grained_question_stats_list)
            fine_grained_question_stats_file = output_dir / "fine_grained_question_statistics.xlsx"
            fine_grained_question_stats_df.to_excel(fine_grained_question_stats_file, index=False)
            safe_print("    Regenerated fine_grained question statistics Excel: {}".format(fine_grained_question_stats_file.name))
        
        if coarse_grained_question_stats_list:
            coarse_grained_question_stats_df = pd.DataFrame(coarse_grained_question_stats_list)
            coarse_grained_question_stats_file = output_dir / "coarse_grained_question_statistics.xlsx"
            coarse_grained_question_stats_df.to_excel(coarse_grained_question_stats_file, index=False)
            safe_print("    Regenerated coarse_grained question statistics Excel: {}".format(coarse_grained_question_stats_file.name))
        
        # é‡æ–°ç”Ÿæˆè¿è¡Œæ±‡æ€»Excel
        run_summary_list = []
        if 'run_files' in multi_run_stats and 'question_statistics' in multi_run_stats:
            for run_dir in multi_run_stats['run_files']:
                total_score = 0
                max_possible = 0
                for question_data in multi_run_stats['question_statistics'].values():
                    if run_dir in question_data.get('runs', {}):
                        total_score += question_data['runs'][run_dir]['earned_score']
                    max_possible += question_data.get('max_points', 0)
                
                score_rate = (total_score / max_possible * 100) if max_possible > 0 else 0
                
                run_summary_list.append({
                    'run_dir': run_dir,
                    'total_score': round(total_score, 4),
                    'max_possible_score': round(max_possible, 4),
                    'score_rate': round(score_rate, 2)
                })
        
        if run_summary_list:
            run_summary_df = pd.DataFrame(run_summary_list)
            run_summary_file = output_dir / "run_summary.xlsx"
            # å¤‡ä»½åŸExcelæ–‡ä»¶
            if run_summary_file.exists():
                backup_excel = run_summary_file.with_suffix('.xlsx.backup')
                shutil.copy2(run_summary_file, backup_excel)
            
            run_summary_df.to_excel(run_summary_file, index=False)
            safe_print("    Regenerated run summary Excel: {}".format(run_summary_file.name))
                
        # except Exception as e:
        #     safe_print("    Excel file regeneration failed: {}".format(e))

    def _regenerate_summary_table(self, base_path):
        """é‡æ–°ç”Ÿæˆæ€»æ±‡æ€»è¡¨æ ¼"""
        # try:
        import shutil
        safe_print("Regenerating summary table...")
        
        # æ”¶é›†æ‰€æœ‰ä¿®å¤åçš„ç»Ÿè®¡æ•°æ®
        summary_table = []
        stat_files = list(base_path.glob("**/*multi_run_statistics.json"))
        
        for stat_file in stat_files:
            # try:
            with open(stat_file, 'r', encoding='utf-8') as f:
                stats = json.load(f)
            
            # è®¡ç®—æ€»ä½“ç»Ÿè®¡ï¼ˆåŸºäºä¿®å¤åçš„earned_scoreï¼‰
            all_scores = []
            total_max_points = 0
            
            if 'question_statistics' in stats:
                for question_data in stats['question_statistics'].values():
                    total_max_points += question_data.get('max_points', 0)
                    if 'runs' in question_data:
                        for run_data in question_data['runs'].values():
                            all_scores.append(run_data['earned_score'])
            
            if all_scores:
                total_score = sum(all_scores)
                mean_score = np.mean(all_scores)
                std_score = np.std(all_scores)
                num_runs = len(stats.get('run_files', []))
                print(f"{total_score=}, {total_max_points=}, {num_runs=}")
                score_rate = (total_score / (total_max_points * num_runs) * 100) if total_max_points > 0 else 0
                
                summary_table.append({
                    'dataset': stats.get('dataset_name', stats.get('dataset_key', 'Unknown')),
                    'num_runs': num_runs,
                    'total_score': round(total_score, 2),
                    'max_possible_score': round(total_max_points * num_runs, 2),
                    'mean_score': round(mean_score, 4),
                    'std_score': round(std_score, 4),
                    'mean_score_rate': round(score_rate, 2),
                    'std_score_rate': round(std_score / total_max_points * 100 if total_max_points > 0 else 0, 2)
                })
                    
            # except Exception as e:
            #     safe_print("    Error processing {}: {}".format(stat_file, e))
        
        # ä¿å­˜æ€»æ±‡æ€»è¡¨æ ¼
        if summary_table:
            summary_table_df = pd.DataFrame(summary_table)
            summary_table_file = base_path / "multi_run_summary_table.xlsx"
            
            # å¤‡ä»½åŸæ–‡ä»¶
            if summary_table_file.exists():
                backup_file = summary_table_file.with_suffix('.xlsx.backup')
                shutil.copy2(summary_table_file, backup_file)
            
            summary_table_df.to_excel(summary_table_file, index=False)
            safe_print("    Summary table updated: {}".format(summary_table_file))
        
        # except Exception as e:
        #     safe_print("    Summary table generation failed: {}".format(e))